{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Sentence Boundary Detection - Decoder Inference Pipeline\n",
    "\n",
    "This notebook provides an inference pipeline for sentence boundary detection in Italian text using decoder-based LLM models with various prompting strategies.\n",
    "\n",
    "## Models\n",
    "\n",
    "The following local models are evaluated:\n",
    "\n",
    "1. **Llama-3.2-1B**: meta-llama/Llama-3.2-1B-Instruct\n",
    "2. **Llama-3.2-3B**: meta-llama/Llama-3.2-3B-Instruct\n",
    "3. **Llama-3.1-8B**: meta-llama/Llama-3.1-8B-Instruct\n",
    "4. **Qwen3-8B**: Qwen/Qwen3-8B\n",
    "\n",
    "## Strategies\n",
    "\n",
    "Seven prompting strategies are implemented:\n",
    "\n",
    "1. **Sliding Window**: Binary YES/NO classification for each punctuation mark\n",
    "2. **Next-Token Probability**: Analyze probability of sentence starters after punctuation\n",
    "3. **Marker Insertion**: Insert <EOS> markers and align back to tokens\n",
    "4. **Structured JSON**: Output boundary indices as JSON\n",
    "5. **Few-Shot Hard**: Few-shot learning with edge case examples\n",
    "6. **Chain-of-Thought**: Step-by-step reasoning for each punctuation\n",
    "7. **Iterative Refinement**: Two-pass prediction with verification\n",
    "\n",
    "## Task\n",
    "\n",
    "Binary token classification:\n",
    "- Label 0: Token does not end a sentence\n",
    "- Label 1: Token ends a sentence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers accelerate bitsandbytes safetensors\n",
    "!pip install -q pandas numpy scikit-learn tqdm huggingface-hub hf-transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n",
      "Number of models to evaluate: 2\n",
      "Number of strategies: 7\n",
      "Output directory: decoder_inference_output\n"
     ]
    }
   ],
   "source": [
    "# Configuration Cell\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"llama-3.2-1b\": {\n",
    "        \"model_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        \"model_type\": \"decoder\"\n",
    "    },\n",
    "    \"llama-3.2-3b\": {\n",
    "        \"model_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        \"model_type\": \"decoder\"\n",
    "    },\n",
    "    \"llama-3.1-8b\": {\n",
    "        \"model_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"model_type\": \"decoder\"\n",
    "    },\n",
    "    \"qwen3-8b\": {\n",
    "        \"model_path\": \"Qwen/Qwen3-8B\",\n",
    "        \"model_type\": \"decoder\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# In this notebook w showcase\n",
    "# MODELS = {\n",
    "#     \"llama-3.2-1b\": {\n",
    "#         \"model_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "#         \"model_type\": \"decoder\"\n",
    "#     },\n",
    "#     \"llama-3.2-3b\": {\n",
    "#         \"model_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "#         \"model_type\": \"decoder\"\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# Strategy configurations\n",
    "STRATEGIES = {\n",
    "    1: \"sliding_window\",\n",
    "    2: \"next_token_prob\",\n",
    "    3: \"marker_insertion\",\n",
    "    4: \"structured_json\",\n",
    "    5: \"few_shot_hard\",\n",
    "    6: \"chain_of_thought\",\n",
    "    7: \"iterative_refinement\",\n",
    "}\n",
    "\n",
    "STRATEGY_NAMES = {\n",
    "    1: \"Sliding Window\",\n",
    "    2: \"Next-Token Prob\",\n",
    "    3: \"Marker Insertion\",\n",
    "    4: \"Structured JSON\",\n",
    "    5: \"Few-Shot Hard\",\n",
    "    6: \"Chain-of-Thought\",\n",
    "    7: \"Iterative Refinement\",\n",
    "}\n",
    "\n",
    "# Inference parameters\n",
    "CHUNK_SIZE = 150\n",
    "BATCH_SIZE = 8\n",
    "MAX_NEW_TOKENS = 300\n",
    "CACHE_DIR = \"cache\"\n",
    "OUTPUT_DIR = \"decoder_inference_output\"\n",
    "\n",
    "# Input file configuration\n",
    "CUSTOM_INPUT_FILE = None\n",
    "DEFAULT_INPUT_FILE = \"OOD_test.csv\"\n",
    "\n",
    "# Team name\n",
    "GROUP_NAME = \"exACSAI\"\n",
    "\n",
    "# HF Token\n",
    "\n",
    "HF_TOKEN = \"\" # Add your HF access token here\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Number of models to evaluate: {len(MODELS)}\")\n",
    "print(f\"Number of strategies: {len(STRATEGIES)}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA RTX A4000\n",
      "Memory: 16.78 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, \n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Login to HF\n",
    "from huggingface_hub import login, whoami\n",
    "login(token=HF_TOKEN)\n",
    "# print(whoami())\n",
    "# Check device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined.\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(filepath):\n",
    "    \"\"\"Load test data from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file with token;label format\n",
    "        \n",
    "    Returns:\n",
    "        tokens: List of tokens\n",
    "        labels: List of labels\n",
    "        df: Raw dataframe\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {filepath}\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Skip header lines\n",
    "    start_idx = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip().replace('\\r', '')\n",
    "        if line.lower().startswith('pinocchio') or line == 'token;label':\n",
    "            start_idx = i + 1\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    for line in lines[start_idx:]:\n",
    "        line = line.strip().replace('\\r', '')\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Handle quoted semicolons\n",
    "        if line.startswith('\";\"'):\n",
    "            tokens.append(';')\n",
    "            parts = line.split(';')\n",
    "            if len(parts) >= 3:\n",
    "                try:\n",
    "                    labels.append(int(parts[2]))\n",
    "                except ValueError:\n",
    "                    labels.append(0)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "            continue\n",
    "        \n",
    "        parts = line.split(';')\n",
    "        if len(parts) >= 2:\n",
    "            token = parts[0].strip('\"')\n",
    "            try:\n",
    "                label = int(parts[1])\n",
    "                tokens.append(token)\n",
    "                labels.append(label)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    df = pd.DataFrame({'token': tokens, 'label': labels})\n",
    "    \n",
    "    print(f\"Loaded {len(tokens)} tokens\")\n",
    "    print(f\"Label distribution: 0={labels.count(0)}, 1={labels.count(1)}\")\n",
    "    \n",
    "    return tokens, labels, df\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Loading Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class defined.\n"
     ]
    }
   ],
   "source": [
    "class DecoderModelInference:\n",
    "    \"\"\"Class for running inference with decoder LLM models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, use_quantization: bool = False):\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        print(f\"Loading model: {model_path}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"left\"\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded successfully\")\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate_batch(self, prompts: List[str], max_new_tokens: int = 256, batch_size: int = 8) -> List[str]:\n",
    "        \"\"\"Batch generation for efficiency.\"\"\"\n",
    "        all_responses = []\n",
    "        \n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch_prompts = prompts[i:i + batch_size]\n",
    "            \n",
    "            # Apply chat template\n",
    "            formatted_prompts = []\n",
    "            for prompt in batch_prompts:\n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                try:\n",
    "                    formatted = self.tokenizer.apply_chat_template(\n",
    "                        messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    )\n",
    "                    formatted_prompts.append(formatted)\n",
    "                except Exception:\n",
    "                    formatted_prompts.append(prompt)\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            # Generate\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            \n",
    "            # Decode responses\n",
    "            for j, output in enumerate(outputs):\n",
    "                input_len = inputs['input_ids'][j].shape[0]\n",
    "                generated = output[input_len:]\n",
    "                response = self.tokenizer.decode(generated, skip_special_tokens=True)\n",
    "                all_responses.append(response.strip())\n",
    "        \n",
    "        return all_responses\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def get_next_token_probs(self, text: str, target_tokens: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Get probability of specific next tokens.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        result = {}\n",
    "        for token in target_tokens:\n",
    "            token_ids = self.tokenizer.encode(token, add_special_tokens=False)\n",
    "            if token_ids:\n",
    "                first_id = token_ids[0]\n",
    "                if first_id < len(probs):\n",
    "                    result[token] = probs[first_id].item()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Free GPU memory.\"\"\"\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "print(\"Model class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompting Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# Common Italian sentence starters for Strategy 2\n",
    "SENTENCE_STARTERS = [\n",
    "    \"Il\", \"La\", \"Lo\", \"Le\", \"Li\", \"I\", \"Gli\",\n",
    "    \"Un\", \"Una\", \"Uno\",\n",
    "    \"E\", \"Ma\", \"Se\", \"Non\", \"Per\", \"Con\", \"Da\", \"Di\", \"A\", \"In\",\n",
    "    \"Quel\", \"Quella\", \"Questo\", \"Questa\", \"Chi\", \"Che\", \"Come\", \"Quando\", \"Dove\",\n",
    "    \"Era\", \"Fu\", \"Si\", \"Ne\", \"Ci\",\n",
    "]\n",
    "\n",
    "# Few-shot examples for Strategy 5\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "### Example 1: Abbreviation (NOT a boundary) ###\n",
    "Tokens: S. Maria della Stella era\n",
    "Labels: 0,0,0,0,0,0\n",
    "\n",
    "### Example 2: True sentence boundary ###\n",
    "Tokens: in nuovi seni . La costiera\n",
    "Labels: 0,0,0,1,0,0\n",
    "\n",
    "### Example 3: Colon before speech (NOT a boundary) ###\n",
    "Tokens: disse : - Non mi\n",
    "Labels: 0,0,0,0,0\n",
    "\n",
    "### Example 4: Semicolon in literary text (NOT a boundary) ###\n",
    "Tokens: era schermito ; pero con\n",
    "Labels: 0,0,0,0,0\n",
    "\n",
    "### Example 5: Question mark (IS a boundary) ###\n",
    "Tokens: Al sagrestano gli crede ? - Perche\n",
    "Labels: 0,0,0,0,1,0,0\n",
    "\"\"\"\n",
    "\n",
    "def get_punctuation_indices(tokens: List[str]) -> List[int]:\n",
    "    \"\"\"Find indices of punctuation marks.\"\"\"\n",
    "    punctuation = {'.', '!', '?', ';', ':', '...', '...'}\n",
    "    indices = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in punctuation:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def parse_model_output(output: str, expected_length: int) -> List[int]:\n",
    "    \"\"\"Parse model output to list of predictions.\"\"\"\n",
    "    predictions = [0] * expected_length\n",
    "    output = output.strip()\n",
    "    \n",
    "    # Look for comma-separated values\n",
    "    lines = output.split('\\n')\n",
    "    candidates = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        clean = line.replace(' ', '').replace(',', '').replace('0', '').replace('1', '')\n",
    "        if len(clean) < len(line) * 0.3:\n",
    "            candidates.append(line)\n",
    "    \n",
    "    if candidates:\n",
    "        output = candidates[-1]\n",
    "    \n",
    "    output = output.replace(' ', '')\n",
    "    parts = output.split(',')\n",
    "    \n",
    "    labels = []\n",
    "    for p in parts:\n",
    "        p = p.strip()\n",
    "        if p in ['0', '1']:\n",
    "            labels.append(int(p))\n",
    "        elif p.startswith('0') or p.startswith('1'):\n",
    "            labels.append(int(p[0]))\n",
    "    \n",
    "    if len(labels) < expected_length:\n",
    "        labels.extend([0] * (expected_length - len(labels)))\n",
    "    elif len(labels) > expected_length:\n",
    "        labels = labels[:expected_length]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "print(\"Strategy utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Strategy Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All strategy functions defined.\n"
     ]
    }
   ],
   "source": [
    "def strategy_1_sliding_window(model: DecoderModelInference, tokens: List[str], batch_size: int = 8) -> List[int]:\n",
    "    \"\"\"Strategy 1: Sliding Window Binary Classification.\"\"\"\n",
    "    predictions = [0] * len(tokens)\n",
    "    punct_indices = get_punctuation_indices(tokens)\n",
    "    \n",
    "    if not punct_indices:\n",
    "        return predictions\n",
    "    \n",
    "    prompts = []\n",
    "    for idx in punct_indices:\n",
    "        start = max(0, idx - 15)\n",
    "        end = min(len(tokens), idx + 16)\n",
    "        context = \" \".join(tokens[start:end])\n",
    "        \n",
    "        prompt = f\"\"\"Analyze Italian text for sentence boundaries.\n",
    "\n",
    "Context: \"{context}\"\n",
    "Token: [{tokens[idx]}]\n",
    "\n",
    "Does this punctuation mark end a sentence? Answer ONLY \"YES\" or \"NO\":\"\"\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    responses = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Strategy 1\"):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_responses = model.generate_batch(batch, max_new_tokens=10, batch_size=len(batch))\n",
    "        responses.extend(batch_responses)\n",
    "    \n",
    "    for idx, response in zip(punct_indices, responses):\n",
    "        response = response.strip().upper()\n",
    "        if \"YES\" in response:\n",
    "            predictions[idx] = 1\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def strategy_2_next_token_prob(\n",
    "    model: DecoderModelInference,\n",
    "    tokens: List[str],\n",
    "    threshold: float = 0.15,\n",
    "    batch_size: int = None,   # <-- add this\n",
    ") -> List[int]:\n",
    "    \"\"\"Strategy 2: Next-Token Probability Analysis.\"\"\"\n",
    "    predictions = [0] * len(tokens)\n",
    "    punct_indices = get_punctuation_indices(tokens)\n",
    "\n",
    "    if not punct_indices:\n",
    "        return predictions\n",
    "\n",
    "    for idx in tqdm(punct_indices, desc=\"Strategy 2\"):\n",
    "        start = max(0, idx - 50)\n",
    "        context = \" \".join(tokens[start:idx + 1])\n",
    "\n",
    "        probs = model.get_next_token_probs(context, SENTENCE_STARTERS)\n",
    "        total_prob = sum(probs.values())\n",
    "\n",
    "        if total_prob > threshold:\n",
    "            predictions[idx] = 1\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def strategy_3_marker_insertion(model: DecoderModelInference, tokens: List[str], chunk_size: int = 200, batch_size: int = 8) -> List[int]:\n",
    "    \"\"\"Strategy 3: Marker Insertion.\"\"\"\n",
    "    predictions = [0] * len(tokens)\n",
    "    EOS_MARKER = \"<EOS>\"\n",
    "    \n",
    "    prompts = []\n",
    "    chunk_ranges = []\n",
    "    \n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        end = min(i + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[i:end]\n",
    "        text = \" \".join(chunk_tokens)\n",
    "        \n",
    "        prompt = f\"\"\"Insert {EOS_MARKER} marker after each sentence-ending token.\n",
    "\n",
    "Input: {text}\n",
    "\n",
    "Output (same text with {EOS_MARKER} markers):\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        chunk_ranges.append((i, end))\n",
    "    \n",
    "    responses = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Strategy 3\"):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_responses = model.generate_batch(batch, max_new_tokens=500, batch_size=len(batch))\n",
    "        responses.extend(batch_responses)\n",
    "    \n",
    "    for (start, end), response in zip(chunk_ranges, responses):\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        for i, token in enumerate(chunk_tokens):\n",
    "            patterns = [f\"{token} {EOS_MARKER}\", f\"{token}{EOS_MARKER}\", \n",
    "                       f\"{token} {EOS_MARKER.lower()}\", f\"{token}{EOS_MARKER.lower()}\"]\n",
    "            for pattern in patterns:\n",
    "                if pattern in response:\n",
    "                    predictions[start + i] = 1\n",
    "                    break\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def strategy_4_structured_json(model: DecoderModelInference, tokens: List[str], chunk_size: int = 150, batch_size: int = 8) -> List[int]:\n",
    "    \"\"\"Strategy 4: Structured JSON Output.\"\"\"\n",
    "    predictions = [0] * len(tokens)\n",
    "    \n",
    "    prompts = []\n",
    "    chunk_ranges = []\n",
    "    \n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        end = min(i + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[i:end]\n",
    "        numbered = [f\"{j}:{t}\" for j, t in enumerate(chunk_tokens)]\n",
    "        token_str = \" \".join(numbered)\n",
    "        \n",
    "        prompt = f\"\"\"Identify sentence boundaries.\n",
    "\n",
    "Tokens: {token_str}\n",
    "\n",
    "Output JSON with boundary indices: {{\"boundaries\": [...]}}:\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        chunk_ranges.append((i, end))\n",
    "    \n",
    "    responses = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Strategy 4\"):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_responses = model.generate_batch(batch, max_new_tokens=400, batch_size=len(batch))\n",
    "        responses.extend(batch_responses)\n",
    "    \n",
    "    for (start, end), response in zip(chunk_ranges, responses):\n",
    "        chunk_len = end - start\n",
    "        \n",
    "        # Try to parse JSON\n",
    "        json_match = re.search(r'\\{[^}]+\\}', response)\n",
    "        if json_match:\n",
    "            try:\n",
    "                data = json.loads(json_match.group())\n",
    "                if \"boundaries\" in data:\n",
    "                    for idx in data[\"boundaries\"]:\n",
    "                        if isinstance(idx, int) and 0 <= idx < chunk_len:\n",
    "                            predictions[start + idx] = 1\n",
    "                    continue\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        # Fallback: find numbers\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "        for num_str in numbers:\n",
    "            idx = int(num_str)\n",
    "            if 0 <= idx < chunk_len:\n",
    "                predictions[start + idx] = 1\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def strategy_5_few_shot_hard(model: DecoderModelInference, tokens: List[str], chunk_size: int = 100, batch_size: int = 8) -> List[int]:\n",
    "    \"\"\"Strategy 5: Few-Shot Learning with Hard Examples.\"\"\"\n",
    "    predictions = [0] * len(tokens)\n",
    "    \n",
    "    prompts = []\n",
    "    chunk_ranges = []\n",
    "    \n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        end = min(i + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[i:end]\n",
    "        token_str = \" \".join(chunk_tokens)\n",
    "        \n",
    "        prompt = f\"\"\"Italian sentence boundary detection.\n",
    "\n",
    "Learn from examples:\n",
    "{FEW_SHOT_EXAMPLES}\n",
    "\n",
    "Tokens: {token_str}\n",
    "\n",
    "Output comma-separated 0s and 1s ({len(chunk_tokens)} values):\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        chunk_ranges.append((i, end))\n",
    "    \n",
    "    responses = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Strategy 5\"):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_responses = model.generate_batch(batch, max_new_tokens=300, batch_size=len(batch))\n",
    "        responses.extend(batch_responses)\n",
    "    \n",
    "    for (start, end), response in zip(chunk_ranges, responses):\n",
    "        chunk_len = end - start\n",
    "        chunk_preds = parse_model_output(response, chunk_len)\n",
    "        predictions[start:end] = chunk_preds\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def strategy_6_chain_of_thought(model: DecoderModelInference, tokens: List[str], batch_size: int = 8) -> List[int]:\n",
    "    \"\"\"Strategy 6: Chain-of-Thought Reasoning.\"\"\"\n",
    "    predictions = [0] * len(tokens)\n",
    "    punct_indices = get_punctuation_indices(tokens)\n",
    "    \n",
    "    if not punct_indices:\n",
    "        return predictions\n",
    "    \n",
    "    prompts = []\n",
    "    for idx in punct_indices:\n",
    "        start = max(0, idx - 20)\n",
    "        end = min(len(tokens), idx + 21)\n",
    "        \n",
    "        before = \" \".join(tokens[start:idx])\n",
    "        punct = tokens[idx]\n",
    "        after = \" \".join(tokens[idx + 1:end]) if idx + 1 < end else \"\"\n",
    "        next_word = tokens[idx + 1] if idx + 1 < len(tokens) else \"END\"\n",
    "        \n",
    "        prompt = f\"\"\"Analyze if this punctuation ends a sentence.\n",
    "\n",
    "Before: \"{before}\"\n",
    "Punctuation: [{punct}]\n",
    "After: \"{after}\"\n",
    "Next word: \"{next_word}\"\n",
    "\n",
    "Think step by step, then answer FINAL: YES or FINAL: NO\"\"\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    responses = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Strategy 6\"):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_responses = model.generate_batch(batch, max_new_tokens=200, batch_size=len(batch))\n",
    "        responses.extend(batch_responses)\n",
    "    \n",
    "    for idx, response in zip(punct_indices, responses):\n",
    "        response = response.upper()\n",
    "        final_match = re.search(r'FINAL:\\s*(YES|NO)', response)\n",
    "        if final_match:\n",
    "            if final_match.group(1) == \"YES\":\n",
    "                predictions[idx] = 1\n",
    "        elif response.strip().endswith(\"YES\"):\n",
    "            predictions[idx] = 1\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def strategy_7_iterative_refinement(model: DecoderModelInference, tokens: List[str], chunk_size: int = 100, batch_size: int = 8) -> List[int]:\n",
    "    \"\"\"Strategy 7: Iterative Refinement.\"\"\"\n",
    "    predictions = [0] * len(tokens)\n",
    "    \n",
    "    # Pass 1: Initial predictions\n",
    "    prompts = []\n",
    "    chunk_ranges = []\n",
    "    \n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        end = min(i + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[i:end]\n",
    "        token_str = \" \".join(chunk_tokens)\n",
    "        \n",
    "        prompt = f\"\"\"Sentence boundary detection.\n",
    "\n",
    "Tokens: {token_str}\n",
    "\n",
    "Output comma-separated 0s and 1s ({len(chunk_tokens)} values):\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        chunk_ranges.append((i, end))\n",
    "    \n",
    "    responses = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Strategy 7 Pass 1\"):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_responses = model.generate_batch(batch, max_new_tokens=250, batch_size=len(batch))\n",
    "        responses.extend(batch_responses)\n",
    "    \n",
    "    for (start, end), response in zip(chunk_ranges, responses):\n",
    "        chunk_len = end - start\n",
    "        chunk_preds = parse_model_output(response, chunk_len)\n",
    "        predictions[start:end] = chunk_preds\n",
    "    \n",
    "    # Pass 2: Verify punctuation predictions\n",
    "    punct_indices = get_punctuation_indices(tokens)\n",
    "    if punct_indices:\n",
    "        verify_prompts = []\n",
    "        verify_indices = []\n",
    "        \n",
    "        for idx in punct_indices[:50]:  # Limit verification\n",
    "            start = max(0, idx - 10)\n",
    "            end = min(len(tokens), idx + 11)\n",
    "            context = \" \".join(tokens[start:end])\n",
    "            pred = \"BOUNDARY\" if predictions[idx] == 1 else \"NOT boundary\"\n",
    "            \n",
    "            prompt = f\"\"\"Verify: Token '{tokens[idx]}' predicted as {pred}.\n",
    "Context: \"{context}\"\n",
    "\n",
    "Is this correct? Answer: CORRECT or INCORRECT (and correct label 0 or 1):\"\"\"\n",
    "            verify_prompts.append(prompt)\n",
    "            verify_indices.append(idx)\n",
    "        \n",
    "        verify_responses = []\n",
    "        for i in tqdm(range(0, len(verify_prompts), batch_size), desc=\"Strategy 7 Pass 2\"):\n",
    "            batch = verify_prompts[i:i + batch_size]\n",
    "            batch_responses = model.generate_batch(batch, max_new_tokens=50, batch_size=len(batch))\n",
    "            verify_responses.extend(batch_responses)\n",
    "        \n",
    "        for idx, response in zip(verify_indices, verify_responses):\n",
    "            response = response.upper()\n",
    "            if \"INCORRECT\" in response:\n",
    "                if \":1\" in response or \"= 1\" in response or \"LABEL 1\" in response:\n",
    "                    predictions[idx] = 1\n",
    "                elif \":0\" in response or \"= 0\" in response or \"LABEL 0\" in response:\n",
    "                    predictions[idx] = 0\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Map strategy IDs to functions\n",
    "STRATEGY_FUNCTIONS = {\n",
    "    1: strategy_1_sliding_window,\n",
    "    2: strategy_2_next_token_prob,\n",
    "    3: strategy_3_marker_insertion,\n",
    "    4: strategy_4_structured_json,\n",
    "    5: strategy_5_few_shot_hard,\n",
    "    6: strategy_6_chain_of_thought,\n",
    "    7: strategy_7_iterative_refinement,\n",
    "}\n",
    "\n",
    "print(\"All strategy functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(predictions, labels):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    class_report = classification_report(\n",
    "        labels, predictions, \n",
    "        target_names=['No Split (0)', 'Split (1)'],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: OOD_test.csv\n",
      "Loaded 1522 tokens\n",
      "Label distribution: 0=1426, 1=96\n",
      "\n",
      "Data loaded successfully.\n",
      "Total tokens: 1522\n",
      "Sentence boundaries: 96\n"
     ]
    }
   ],
   "source": [
    "# Determine input file\n",
    "input_file = CUSTOM_INPUT_FILE if CUSTOM_INPUT_FILE else DEFAULT_INPUT_FILE\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Input file not found: {input_file}\")\n",
    "    print(\"Please ensure the OOD_test.csv file is in the current directory.\")\n",
    "else:\n",
    "    tokens, labels, raw_df = load_test_data(input_file)\n",
    "    print(f\"\\nData loaded successfully.\")\n",
    "    print(f\"Total tokens: {len(tokens)}\")\n",
    "    print(f\"Sentence boundaries: {sum(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Inference for All Models and Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading model: llama-3.2-1b\n",
      "================================================================================\n",
      "Loading model: meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "\n",
      "--- Running Strategy 1: Sliding Window ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9280ecba49a645ef83257dbe0e5ad5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 1:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9396\n",
      "  Precision: 0.6000\n",
      "  Recall:    0.1250\n",
      "  F1 Score:  0.2069\n",
      "\n",
      "--- Running Strategy 2: Next-Token Prob ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c5883e0aa44fb4a1dcb552b1341b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 2:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9369\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  F1 Score:  0.0000\n",
      "\n",
      "--- Running Strategy 3: Marker Insertion ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b128f4443246798cfb6e8a52d70541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9021\n",
      "  Precision: 0.1807\n",
      "  Recall:    0.1562\n",
      "  F1 Score:  0.1676\n",
      "\n",
      "--- Running Strategy 4: Structured JSON ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dbc76ad8024c7e9144be80a190715f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 4:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.6367\n",
      "  Precision: 0.0792\n",
      "  Recall:    0.4479\n",
      "  F1 Score:  0.1346\n",
      "\n",
      "--- Running Strategy 5: Few-Shot Hard ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7433243925b43538551a922c69663b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 5:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9369\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  F1 Score:  0.0000\n",
      "\n",
      "--- Running Strategy 6: Chain-of-Thought ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd3aca9ae9d4310bc68a6d887f619e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 6:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9435\n",
      "  Precision: 0.7500\n",
      "  Recall:    0.1562\n",
      "  F1 Score:  0.2586\n",
      "\n",
      "--- Running Strategy 7: Iterative Refinement ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d881876cb247588cee3f7a1cca50f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 7 Pass 1:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685dafe96c1d46e4a646c60a7f9cd0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 7 Pass 2:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.8955\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  F1 Score:  0.0000\n",
      "\n",
      "================================================================================\n",
      "Loading model: llama-3.2-3b\n",
      "================================================================================\n",
      "Loading model: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952f2e2a1fbf448cb74a09c7a3b19ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012e9bd98b3a473c9774e875393daa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de84ede2c6c24954be9afd862d5f7074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87998d3563f44335beea43bef5cd9304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865fd07adb8a4d5eae340913fb5670b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a4752f80904e6c8ce889c81bf99865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4612dd4bff746b794b18da30556ab48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184fe6135daf4776a439edb7cfd71d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491eb35b9fcf4b6a80c473e59b8ac679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28317e8bd00b4f6ba3e6fde14780fa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "\n",
      "--- Running Strategy 1: Sliding Window ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996eff96afd94c268626076d71878732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 1:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9369\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  F1 Score:  0.0000\n",
      "\n",
      "--- Running Strategy 2: Next-Token Prob ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e3a62b7a944c9cbb38bb44f98e3d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 2:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9369\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "  F1 Score:  0.0000\n",
      "\n",
      "--- Running Strategy 3: Marker Insertion ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788bebc8fe6e4070bdf0924f45e93d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9488\n",
      "  Precision: 0.7045\n",
      "  Recall:    0.3229\n",
      "  F1 Score:  0.4429\n",
      "\n",
      "--- Running Strategy 4: Structured JSON ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7aecb37f434061ac7ed1bb73993c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 4:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.6846\n",
      "  Precision: 0.0556\n",
      "  Recall:    0.2500\n",
      "  F1 Score:  0.0909\n",
      "\n",
      "--- Running Strategy 5: Few-Shot Hard ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84958c81c8646c496073e543a4dd678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 5:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9363\n",
      "  Precision: 0.3333\n",
      "  Recall:    0.0104\n",
      "  F1 Score:  0.0202\n",
      "\n",
      "--- Running Strategy 6: Chain-of-Thought ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e389e221d7843818244d22a459ed4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 6:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.9396\n",
      "  Precision: 0.8333\n",
      "  Recall:    0.0521\n",
      "  F1 Score:  0.0980\n",
      "\n",
      "--- Running Strategy 7: Iterative Refinement ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e00960959f47c9ae63f5fa998eef45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 7 Pass 1:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98856c431fc4b548ad6911e08a24b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Strategy 7 Pass 2:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy:  0.5953\n",
      "  Precision: 0.0681\n",
      "  Recall:    0.4271\n",
      "  F1 Score:  0.1175\n",
      "\n",
      "================================================================================\n",
      "All models and strategies evaluated.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Store results\n",
    "results = []\n",
    "all_predictions = {}  # {(model_name, strategy_id): predictions}\n",
    "\n",
    "for model_name, model_config in MODELS.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        model = DecoderModelInference(model_config['model_path'])\n",
    "        \n",
    "        for strategy_id, strategy_name in STRATEGIES.items():\n",
    "            print(f\"\\n--- Running Strategy {strategy_id}: {STRATEGY_NAMES[strategy_id]} ---\")\n",
    "            \n",
    "            try:\n",
    "                strategy_func = STRATEGY_FUNCTIONS[strategy_id]\n",
    "                predictions = strategy_func(model, tokens, batch_size=BATCH_SIZE)\n",
    "                \n",
    "                # Compute metrics\n",
    "                metrics = compute_metrics(predictions, labels)\n",
    "                \n",
    "                print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "                print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "                print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "                print(f\"  F1 Score:  {metrics['f1']:.4f}\")\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'model_name': model_name,\n",
    "                    'model_type': 'decoder',\n",
    "                    'strategy_id': strategy_id,\n",
    "                    'strategy_name': STRATEGY_NAMES[strategy_id],\n",
    "                    'accuracy': metrics['accuracy'],\n",
    "                    'precision': metrics['precision'],\n",
    "                    'recall': metrics['recall'],\n",
    "                    'f1': metrics['f1'],\n",
    "                    'huggingface_link': f\"https://huggingface.co/{model_config['model_path']}\"\n",
    "                })\n",
    "                \n",
    "                all_predictions[(model_name, strategy_id)] = predictions\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error in strategy {strategy_id}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Cleanup model\n",
    "        model.cleanup()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All models and strategies evaluated.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "RESULTS SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "  model_name        strategy_name  accuracy  precision   recall       f1\n",
      "llama-3.2-3b     Marker Insertion  0.948752   0.704545 0.322917 0.442857\n",
      "llama-3.2-1b     Chain-of-Thought  0.943495   0.750000 0.156250 0.258621\n",
      "llama-3.2-1b       Sliding Window  0.939553   0.600000 0.125000 0.206897\n",
      "llama-3.2-1b     Marker Insertion  0.902102   0.180723 0.156250 0.167598\n",
      "llama-3.2-1b      Structured JSON  0.636662   0.079190 0.447917 0.134585\n",
      "llama-3.2-3b Iterative Refinement  0.595269   0.068106 0.427083 0.117479\n",
      "llama-3.2-3b     Chain-of-Thought  0.939553   0.833333 0.052083 0.098039\n",
      "llama-3.2-3b      Structured JSON  0.684625   0.055556 0.250000 0.090909\n",
      "llama-3.2-3b        Few-Shot Hard  0.936268   0.333333 0.010417 0.020202\n",
      "llama-3.2-1b      Next-Token Prob  0.936925   0.000000 0.000000 0.000000\n",
      "llama-3.2-1b        Few-Shot Hard  0.936925   0.000000 0.000000 0.000000\n",
      "llama-3.2-1b Iterative Refinement  0.895532   0.000000 0.000000 0.000000\n",
      "llama-3.2-3b       Sliding Window  0.936925   0.000000 0.000000 0.000000\n",
      "llama-3.2-3b      Next-Token Prob  0.936925   0.000000 0.000000 0.000000\n",
      "\n",
      "Summary saved to: decoder_inference_output/decoder_evaluation_summary_20251222_124051.csv\n",
      "\n",
      "Best Performing Configuration:\n",
      "  Model: llama-3.2-3b\n",
      "  Strategy: Marker Insertion\n",
      "  F1 Score: 0.4429\n",
      "  Accuracy: 0.9488\n",
      "  Precision: 0.7045\n",
      "  Recall: 0.3229\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('f1', ascending=False)\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    print(results_df[['model_name', 'strategy_name', 'accuracy', 'precision', 'recall', 'f1']].to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Save summary CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    summary_file = os.path.join(OUTPUT_DIR, f\"decoder_evaluation_summary_{timestamp}.csv\")\n",
    "    results_df.to_csv(summary_file, index=False)\n",
    "    print(f\"Summary saved to: {summary_file}\")\n",
    "    \n",
    "    # Display best result\n",
    "    print(\"\\nBest Performing Configuration:\")\n",
    "    best = results_df.iloc[0]\n",
    "    print(f\"  Model: {best['model_name']}\")\n",
    "    print(f\"  Strategy: {best['strategy_name']}\")\n",
    "    print(f\"  F1 Score: {best['f1']:.4f}\")\n",
    "    print(f\"  Accuracy: {best['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best['recall']:.4f}\")\n",
    "else:\n",
    "    print(\"No results generated. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Prediction Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction output files...\n",
      "\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_1b-sliding_window.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_1b-next_token_prob.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_1b-marker_insertion.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_1b-structured_json.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_1b-few_shot_hard.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_1b-chain_of_thought.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_1b-iterative_refinement.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_3b-sliding_window.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_3b-next_token_prob.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_3b-marker_insertion.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_3b-structured_json.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_3b-few_shot_hard.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_3b-chain_of_thought.csv\n",
      "Saved: decoder_inference_output/exACSAI-hw2_split-llama_3.2_3b-iterative_refinement.csv\n",
      "\n",
      "All prediction files generated successfully.\n",
      "\n",
      "Total prediction files: 14\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating prediction output files...\\n\")\n",
    "\n",
    "prediction_files = []\n",
    "\n",
    "for (model_name, strategy_id), predictions in all_predictions.items():\n",
    "    strategy_name = STRATEGIES[strategy_id]\n",
    "    \n",
    "    # Ensure predictions match token count\n",
    "    if len(predictions) != len(tokens):\n",
    "        print(f\"Warning: {model_name}-{strategy_name} length mismatch. Adjusting...\")\n",
    "        if len(predictions) < len(tokens):\n",
    "            predictions = predictions + [0] * (len(tokens) - len(predictions))\n",
    "        else:\n",
    "            predictions = predictions[:len(tokens)]\n",
    "    \n",
    "    # Create output dataframe\n",
    "    output_df = pd.DataFrame({\n",
    "        'token': tokens,\n",
    "        'label': predictions\n",
    "    })\n",
    "    \n",
    "    # Save with naming convention: groupname-hw2_split-modelname-strategy.csv\n",
    "    safe_model_name = model_name.replace('/', '_').replace('-', '_')\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"{GROUP_NAME}-hw2_split-{safe_model_name}-{strategy_name}.csv\")\n",
    "    output_df.to_csv(output_file, sep=',', index=False)\n",
    "    prediction_files.append(output_file)\n",
    "    print(f\"Saved: {output_file}\")\n",
    "\n",
    "print(f\"\\nAll prediction files generated successfully.\")\n",
    "print(f\"\\nTotal prediction files: {len(prediction_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Best Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model predictions saved to: decoder_inference_output/exACSAI-hw2_split-decoder-best.csv\n",
      "Best configuration: llama-3.2-3b + Marker Insertion\n",
      "F1 Score: 0.4429\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    # Get best configuration\n",
    "    best_result = results_df.iloc[0]\n",
    "    best_model = best_result['model_name']\n",
    "    best_strategy = best_result['strategy_id']\n",
    "    best_strategy_name = STRATEGIES[best_strategy]\n",
    "    \n",
    "    best_predictions = all_predictions.get((best_model, best_strategy))\n",
    "    \n",
    "    if best_predictions is not None:\n",
    "        # Save best model predictions with special naming\n",
    "        output_df = pd.DataFrame({\n",
    "            'token': tokens,\n",
    "            'label': best_predictions\n",
    "        })\n",
    "        \n",
    "        best_file = os.path.join(OUTPUT_DIR, f\"{GROUP_NAME}-hw2_split-decoder-best.csv\")\n",
    "        output_df.to_csv(best_file, sep=',', index=False)\n",
    "        print(f\"Best model predictions saved to: {best_file}\")\n",
    "        print(f\"Best configuration: {best_model} + {STRATEGY_NAMES[best_strategy]}\")\n",
    "        print(f\"F1 Score: {best_result['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "The decoder inference pipeline has completed. The following outputs are available in the `decoder_inference_output` directory:\n",
    "\n",
    "1. **Evaluation Summary**: CSV file with metrics for all model-strategy combinations\n",
    "2. **Prediction Files**: CSV files with predicted labels for each model-strategy pair\n",
    "3. **Best Model Predictions**: CSV file with predictions from the best performing configuration\n",
    "\n",
    "The configurations are ranked by F1 score, with the best performing combination identified above.\n",
    "\n",
    "### Strategy Descriptions\n",
    "\n",
    "1. **Sliding Window**: Binary YES/NO classification for each punctuation mark with context window\n",
    "2. **Next-Token Probability**: Analyzes probability of sentence-starting tokens after punctuation\n",
    "3. **Marker Insertion**: Asks LLM to insert <EOS> markers, then aligns back to tokens\n",
    "4. **Structured JSON**: Outputs boundary indices as JSON object\n",
    "5. **Few-Shot Hard**: Uses curated examples covering edge cases (abbreviations, dialogue, etc.)\n",
    "6. **Chain-of-Thought**: Step-by-step reasoning for each punctuation mark\n",
    "7. **Iterative Refinement**: Two-pass approach with verification and correction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
