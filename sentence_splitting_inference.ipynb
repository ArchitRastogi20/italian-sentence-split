{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Sentence Boundary Detection - Inference Pipeline\n",
    "\n",
    "This notebook provides an inference pipeline for sentence boundary detection in Italian text using fine-tuned encoder models with and without CRF layers.\n",
    "\n",
    "## Models\n",
    "\n",
    "The following models are evaluated:\n",
    "\n",
    "1. **BERT-Italian-CRF**: dbmdz/bert-base-italian-xxl-cased with CRF layer\n",
    "2. **XLM-RoBERTa-CRF**: FacebookAI/xlm-roberta-base with CRF layer  \n",
    "3. **BERT-Italian-Base**: dbmdz/bert-base-italian-xxl-cased (standard encoder)\n",
    "4. **XGBoost**: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 with a Xgboost model for binary classifiaction\n",
    "\n",
    "## Task\n",
    "\n",
    "Binary token classification:\n",
    "- Label 0: Token does not end a sentence\n",
    "- Label 1: Token ends a sentence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers pytorch-crf scikit-learn pandas numpy tqdm huggingface-hub safetensors protobuf xgboost sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n",
      "Number of models to evaluate: 4\n",
      "Output directory: inference_output\n"
     ]
    }
   ],
   "source": [
    "# Configuration Cell\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"bert-italian-crf\": {\n",
    "        \"model_path\": \"ArchitRastogi/bert-base-italian-xxl-cased-sentence-splitter-CRF\",\n",
    "        \"base_model\": \"dbmdz/bert-base-italian-xxl-cased\",\n",
    "        \"is_crf\": True,\n",
    "        \"model_type\": \"transformer\"\n",
    "    },\n",
    "    \"xlm-roberta-crf\": {\n",
    "        \"model_path\": \"ArchitRastogi/xlm-roberta-base-italian-sentence-splitter-CRF\",\n",
    "        \"base_model\": \"FacebookAI/xlm-roberta-base\",\n",
    "        \"is_crf\": True,\n",
    "        \"model_type\": \"transformer\"\n",
    "    },\n",
    "    \"bert-italian-base\": {\n",
    "        \"model_path\": \"ArchitRastogi/bert-base-italian-xxl-cased-sentence-splitter-base\",\n",
    "        \"base_model\": \"dbmdz/bert-base-italian-xxl-cased\",\n",
    "        \"is_crf\": False,\n",
    "        \"model_type\": \"transformer\"\n",
    "    },\n",
    "    \"xgboost\": {\n",
    "        \"model_path\": \"ArchitRastogi/xgboost_sentence_splitting\",\n",
    "        \"embedding_model\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"model_type\": \"xgboost\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Inference parameters\n",
    "MAX_LENGTH = 512\n",
    "STRIDE = 64\n",
    "BATCH_SIZE = 16\n",
    "CACHE_DIR = \"cache\"\n",
    "OUTPUT_DIR = \"inference_output\"\n",
    "\n",
    "# Input file configuration\n",
    "CUSTOM_INPUT_FILE = None\n",
    "DEFAULT_INPUT_FILE = \"OOD_test.csv\"\n",
    "\n",
    "# Team name\n",
    "GROUP_NAME = \"exACSAI\"\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Number of models to evaluate: {len(MODELS)}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA RTX A4500\n",
      "Memory: 21.15 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, \n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# XGBoost imports\n",
    "import xgboost as xgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Check device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model Architecture\n",
    "\n",
    "The CRF wrapper adds a Conditional Random Field layer on top of the encoder for structured prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architectures defined.\n"
     ]
    }
   ],
   "source": [
    "class BERTWithCRF(nn.Module):\n",
    "    \"\"\"BERT encoder with CRF layer for token classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.classifier(sequence_output)\n",
    "        \n",
    "        mask = attention_mask.bool()\n",
    "        \n",
    "        if labels is not None:\n",
    "            labels_crf = labels.clone()\n",
    "            valid_labels_mask = (labels != -100)\n",
    "            labels_crf[labels_crf == -100] = 0\n",
    "            \n",
    "            log_likelihood = self.crf(emissions, labels_crf, mask=mask, reduction='none')\n",
    "            \n",
    "            batch_size = input_ids.size(0)\n",
    "            masked_log_likelihood = []\n",
    "            for i in range(batch_size):\n",
    "                n_valid = valid_labels_mask[i].sum()\n",
    "                if n_valid > 0:\n",
    "                    masked_log_likelihood.append(log_likelihood[i])\n",
    "            \n",
    "            if len(masked_log_likelihood) > 0:\n",
    "                loss = -torch.stack(masked_log_likelihood).mean()\n",
    "            else:\n",
    "                loss = -log_likelihood.mean()\n",
    "            \n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return {'loss': loss, 'logits': emissions, 'predictions': predictions}\n",
    "        else:\n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return {'logits': emissions, 'predictions': predictions}\n",
    "\n",
    "\n",
    "def create_xgboost_features(sentences, labels, embedding_model):\n",
    "    \"\"\"Create features for XGBoost using embeddings and hand-crafted features.\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for sent, labs in tqdm(zip(sentences, labels), desc=\"Creating XGBoost features\", total=len(sentences)):\n",
    "        for i, (token, label) in enumerate(zip(sent, labs)):\n",
    "            # Context window: 2 tokens before, current, 2 after\n",
    "            context_start = max(0, i - 2)\n",
    "            context_end = min(len(sent), i + 3)\n",
    "            context = ' '.join(sent[context_start:context_end])\n",
    "            \n",
    "            # Get embedding\n",
    "            emb = embedding_model.encode(context, show_progress_bar=False)\n",
    "            \n",
    "            # Hand-crafted features\n",
    "            features = [\n",
    "                1 if token == '.' else 0,\n",
    "                1 if token == '!' else 0,\n",
    "                1 if token == '?' else 0,\n",
    "                1 if token == ',' else 0,\n",
    "                1 if token == ';' else 0,\n",
    "                1 if token == ':' else 0,\n",
    "                1 if i + 1 < len(sent) and len(sent[i+1]) > 0 and sent[i + 1][0].isupper() else 0,\n",
    "                1 if i > 0 and len(sent[i-1]) > 0 and sent[i - 1][0].isupper() else 0,\n",
    "                len(token),\n",
    "                i / len(sent),\n",
    "            ]\n",
    "            \n",
    "            X.append(np.concatenate([emb, features]))\n",
    "            y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "print(\"Model architectures defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading Functions\n",
    "\n",
    "Expected CSV format (semicolon-separated):\n",
    "```\n",
    "token;label\n",
    "C';0\n",
    "era;0\n",
    "una;0\n",
    "volta;0\n",
    ".;1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined.\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(filepath):\n",
    "    \"\"\"Load test data from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file with token;label format\n",
    "        \n",
    "    Returns:\n",
    "        sentences: List of token lists\n",
    "        labels: List of label lists\n",
    "        df: Raw dataframe\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {filepath}\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        \n",
    "        # Skip header if present\n",
    "        if first_line.lower().startswith('pinocchio') or first_line == 'token;label':\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split(';')\n",
    "                if len(parts) == 2:\n",
    "                    token, label = parts\n",
    "                    if token == 'token' and label == 'label':\n",
    "                        continue\n",
    "                    try:\n",
    "                        data.append([token, int(label)])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        else:\n",
    "            # First line is data\n",
    "            parts = first_line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                token, label = parts\n",
    "                try:\n",
    "                    data.append([token, int(label)])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split(';')\n",
    "                if len(parts) == 2:\n",
    "                    token, label = parts\n",
    "                    try:\n",
    "                        data.append([token, int(label)])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['token', 'label'])\n",
    "    \n",
    "    # Group into sentences (split at label=1)\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_sent = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        token = str(row['token'])\n",
    "        label = int(row['label'])\n",
    "        current_sent.append(token)\n",
    "        current_labels.append(label)\n",
    "        if label == 1:\n",
    "            sentences.append(current_sent)\n",
    "            labels.append(current_labels)\n",
    "            current_sent = []\n",
    "            current_labels = []\n",
    "    \n",
    "    if current_sent:\n",
    "        sentences.append(current_sent)\n",
    "        labels.append(current_labels)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} tokens grouped into {len(sentences)} sentences\")\n",
    "    print(f\"Label distribution:\\n{df['label'].value_counts().sort_index()}\")\n",
    "    \n",
    "    return sentences, labels, df\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined.\n"
     ]
    }
   ],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    \"\"\"Dataset for inference with stride support.\"\"\"\n",
    "    \n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512, stride=64):\n",
    "        self.encodings = []\n",
    "        self.labels_aligned = []\n",
    "        self.original_indices = []\n",
    "        \n",
    "        for idx, (sent, labs) in enumerate(tqdm(zip(sentences, labels), \n",
    "                                                 desc=\"Tokenizing\", \n",
    "                                                 total=len(sentences))):\n",
    "            text = ' '.join(sent)\n",
    "            \n",
    "            encoding = tokenizer(\n",
    "                text, \n",
    "                truncation=True, \n",
    "                max_length=max_length,\n",
    "                stride=stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "            \n",
    "            for i in range(len(encoding['input_ids'])):\n",
    "                self.encodings.append({\n",
    "                    'input_ids': encoding['input_ids'][i],\n",
    "                    'attention_mask': encoding['attention_mask'][i]\n",
    "                })\n",
    "                \n",
    "                word_ids = encoding.word_ids(batch_index=i)\n",
    "                label_ids = []\n",
    "                previous_word_idx = None\n",
    "                \n",
    "                for word_idx in word_ids:\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:\n",
    "                        label_ids.append(labs[word_idx] if word_idx < len(labs) else 0)\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                \n",
    "                self.labels_aligned.append(label_ids)\n",
    "                self.original_indices.append(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        item['labels'] = torch.tensor(self.labels_aligned[idx])\n",
    "        item['idx'] = self.original_indices[idx]\n",
    "        return item\n",
    "\n",
    "print(\"Dataset class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference functions defined.\n"
     ]
    }
   ],
   "source": [
    "def run_inference(model, dataloader, is_crf=False):\n",
    "    \"\"\"Run inference on dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        dataloader: DataLoader instance\n",
    "        is_crf: Whether model uses CRF layer\n",
    "        \n",
    "    Returns:\n",
    "        predictions: Numpy array of predictions\n",
    "        labels: Numpy array of true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Running inference\"):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            if is_crf:\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                predictions = outputs['predictions']\n",
    "                \n",
    "                for i, pred_seq in enumerate(predictions):\n",
    "                    mask = (labels[i] != -100).cpu().numpy()\n",
    "                    pred_seq_padded = pred_seq + [0] * (len(mask) - len(pred_seq))\n",
    "                    pred_masked = np.array(pred_seq_padded)[mask]\n",
    "                    label_masked = labels[i].cpu().numpy()[mask]\n",
    "                    all_predictions.extend(pred_masked.tolist())\n",
    "                    all_labels.extend(label_masked.tolist())\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                for i in range(predictions.shape[0]):\n",
    "                    mask = (labels[i] != -100).cpu().numpy()\n",
    "                    pred_masked = predictions[i].cpu().numpy()[mask]\n",
    "                    label_masked = labels[i].cpu().numpy()[mask]\n",
    "                    all_predictions.extend(pred_masked.tolist())\n",
    "                    all_labels.extend(label_masked.tolist())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    \"\"\"Compute evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted labels\n",
    "        labels: True labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    class_report = classification_report(\n",
    "        labels, predictions, \n",
    "        target_names=['No Split (0)', 'Split (1)'],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "print(\"Inference functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Loading and Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model testing function defined.\n"
     ]
    }
   ],
   "source": [
    "def test_model(model_name, model_config, sentences, labels):\n",
    "    \"\"\"Load and test a single model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name identifier for the model\n",
    "        model_config: Configuration dictionary\n",
    "        sentences: List of sentence token lists\n",
    "        labels: List of label lists\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of results or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {model_name}\")\n",
    "    print(f\"Model: {model_config['model_path']}\")\n",
    "    print(f\"Type: {model_config['model_type']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Handle XGBoost model differently\n",
    "        if model_config['model_type'] == 'xgboost':\n",
    "            from huggingface_hub import hf_hub_download\n",
    "            \n",
    "            # Load embedding model\n",
    "            print(f\"Loading embedding model: {model_config['embedding_model']}\")\n",
    "            embedding_model = SentenceTransformer(model_config['embedding_model'])\n",
    "            embedding_model = embedding_model.to(DEVICE)\n",
    "            print(\"Embedding model loaded\")\n",
    "            \n",
    "            # Download XGBoost model\n",
    "            print(\"Downloading XGBoost model...\")\n",
    "            model_file = hf_hub_download(\n",
    "                repo_id=model_config['model_path'],\n",
    "                filename=\"xgboost_model.json\",\n",
    "                cache_dir=CACHE_DIR\n",
    "            )\n",
    "            \n",
    "            # Load XGBoost model\n",
    "            xgb_model = xgb.Booster()\n",
    "            xgb_model.load_model(model_file)\n",
    "            print(\"XGBoost model loaded\")\n",
    "            \n",
    "            # Create features\n",
    "            print(\"Creating features...\")\n",
    "            X_test, y_test = create_xgboost_features(sentences, labels, embedding_model)\n",
    "            print(f\"Feature shape: {X_test.shape}\")\n",
    "            \n",
    "            # Run inference\n",
    "            print(\"Running inference...\")\n",
    "            dmatrix = xgb.DMatrix(X_test)\n",
    "            y_pred_proba = xgb_model.predict(dmatrix)\n",
    "            predictions = (y_pred_proba > 0.5).astype(int)\n",
    "            true_labels = y_test\n",
    "            \n",
    "            # Cleanup\n",
    "            del embedding_model, xgb_model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        else:\n",
    "            # Transformer model loading (existing code)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_config['base_model'],\n",
    "                cache_dir=CACHE_DIR,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(f\"Tokenizer loaded: {model_config['base_model']}\")\n",
    "            \n",
    "            if model_config['is_crf']:\n",
    "                from huggingface_hub import hf_hub_download\n",
    "                from safetensors.torch import load_file\n",
    "                \n",
    "                base_encoder = AutoModel.from_pretrained(\n",
    "                    model_config['base_model'],\n",
    "                    cache_dir=CACHE_DIR,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                print(\"Base encoder loaded\")\n",
    "                \n",
    "                model = BERTWithCRF(base_encoder, num_labels=2)\n",
    "                print(\"CRF wrapper created\")\n",
    "                \n",
    "                try:\n",
    "                    model_file = hf_hub_download(\n",
    "                        repo_id=model_config['model_path'],\n",
    "                        filename=\"model.safetensors\",\n",
    "                        cache_dir=CACHE_DIR\n",
    "                    )\n",
    "                    state_dict = load_file(model_file)\n",
    "                    model.load_state_dict(state_dict, strict=False)\n",
    "                    print(\"Weights loaded from safetensors\")\n",
    "                except:\n",
    "                    model_file = hf_hub_download(\n",
    "                        repo_id=model_config['model_path'],\n",
    "                        filename=\"pytorch_model.bin\",\n",
    "                        cache_dir=CACHE_DIR\n",
    "                    )\n",
    "                    state_dict = torch.load(model_file, map_location=DEVICE)\n",
    "                    model.load_state_dict(state_dict, strict=False)\n",
    "                    print(\"Weights loaded from pytorch_model.bin\")\n",
    "            else:\n",
    "                model = AutoModelForTokenClassification.from_pretrained(\n",
    "                    model_config['model_path'],\n",
    "                    cache_dir=CACHE_DIR,\n",
    "                    num_labels=2,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                print(\"Model loaded directly\")\n",
    "            \n",
    "            model = model.to(DEVICE)\n",
    "            model.eval()\n",
    "            print(f\"Model moved to {DEVICE}\")\n",
    "            \n",
    "            test_dataset = InferenceDataset(sentences, labels, tokenizer, MAX_LENGTH, STRIDE)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            predictions, true_labels = run_inference(model, test_loader, is_crf=model_config['is_crf'])\n",
    "            \n",
    "            del model, tokenizer, test_dataset, test_loader\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Compute metrics (same for both model types)\n",
    "        metrics = compute_metrics(predictions, true_labels)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {metrics['f1']:.4f}\")\n",
    "        print(f\"\\nClassification Report:\\n{metrics['classification_report']}\")\n",
    "        print(f\"\\nConfusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'model_path': model_config['model_path'],\n",
    "            'model_type': model_config['model_type'],\n",
    "            'huggingface_link': f\"https://huggingface.co/{model_config['model_path']}\",\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1': metrics['f1'],\n",
    "            'predictions': predictions,\n",
    "            'classification_report': metrics['classification_report']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing model {model_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"Model testing function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Input Data\n",
    "\n",
    "Loads either the custom input file or the default OOD test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: OOD_test.csv\n",
      "\n",
      "Loading data from: OOD_test.csv\n",
      "Loaded 1508 tokens grouped into 88 sentences\n",
      "Label distribution:\n",
      "label\n",
      "0    1420\n",
      "1      88\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset loaded successfully.\n",
      "Total tokens: 1508\n",
      "Total sentences: 88\n"
     ]
    }
   ],
   "source": [
    "# Determine input file\n",
    "input_file = CUSTOM_INPUT_FILE if CUSTOM_INPUT_FILE else DEFAULT_INPUT_FILE\n",
    "\n",
    "print(f\"Input file: {input_file}\")\n",
    "print()\n",
    "\n",
    "# Load data\n",
    "sentences, labels, raw_df = load_test_data(input_file)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully.\")\n",
    "print(f\"Total tokens: {len(raw_df)}\")\n",
    "print(f\"Total sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Inference on All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing: bert-italian-crf\n",
      "Model: ArchitRastogi/bert-base-italian-xxl-cased-sentence-splitter-CRF\n",
      "Type: transformer\n",
      "================================================================================\n",
      "Tokenizer loaded: dbmdz/bert-base-italian-xxl-cased\n",
      "Base encoder loaded\n",
      "CRF wrapper created\n",
      "Weights loaded from safetensors\n",
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013ecb9e39f145c798a17708a0093a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834be5964bc144f6a79460eb32d212cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy:  0.9552\n",
      "  Precision: 0.5938\n",
      "  Recall:    0.6477\n",
      "  F1 Score:  0.6196\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Split (0)     0.9789    0.9736    0.9762      1475\n",
      "   Split (1)     0.5938    0.6477    0.6196        88\n",
      "\n",
      "    accuracy                         0.9552      1563\n",
      "   macro avg     0.7863    0.8106    0.7979      1563\n",
      "weighted avg     0.9572    0.9552    0.9561      1563\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1436   39]\n",
      " [  31   57]]\n",
      "Classification report saved: inference_output/bert-italian-crf_classification_report.txt\n",
      "\n",
      "================================================================================\n",
      "Testing: xlm-roberta-crf\n",
      "Model: ArchitRastogi/xlm-roberta-base-italian-sentence-splitter-CRF\n",
      "Type: transformer\n",
      "================================================================================\n",
      "Tokenizer loaded: FacebookAI/xlm-roberta-base\n",
      "Base encoder loaded\n",
      "CRF wrapper created\n",
      "Weights loaded from safetensors\n",
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cef7df163d4ecead3835a5d53fefab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738a61f8fcd940bbac551ae6ed1cd306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy:  0.9821\n",
      "  Precision: 0.7699\n",
      "  Recall:    0.9886\n",
      "  F1 Score:  0.8657\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Split (0)     0.9993    0.9817    0.9904      1420\n",
      "   Split (1)     0.7699    0.9886    0.8657        88\n",
      "\n",
      "    accuracy                         0.9821      1508\n",
      "   macro avg     0.8846    0.9852    0.9280      1508\n",
      "weighted avg     0.9859    0.9821    0.9831      1508\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1394   26]\n",
      " [   1   87]]\n",
      "Classification report saved: inference_output/xlm-roberta-crf_classification_report.txt\n",
      "\n",
      "================================================================================\n",
      "Testing: bert-italian-base\n",
      "Model: ArchitRastogi/bert-base-italian-xxl-cased-sentence-splitter-base\n",
      "Type: transformer\n",
      "================================================================================\n",
      "Tokenizer loaded: dbmdz/bert-base-italian-xxl-cased\n",
      "Model loaded directly\n",
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6535dd87e84fee8551f8d4fb7d064c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4b803e371d415f891d451b8a902d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy:  0.9629\n",
      "  Precision: 0.6630\n",
      "  Recall:    0.6932\n",
      "  F1 Score:  0.6778\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Split (0)     0.9816    0.9790    0.9803      1475\n",
      "   Split (1)     0.6630    0.6932    0.6778        88\n",
      "\n",
      "    accuracy                         0.9629      1563\n",
      "   macro avg     0.8223    0.8361    0.8290      1563\n",
      "weighted avg     0.9637    0.9629    0.9633      1563\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1444   31]\n",
      " [  27   61]]\n",
      "Classification report saved: inference_output/bert-italian-base_classification_report.txt\n",
      "\n",
      "================================================================================\n",
      "Testing: xgboost\n",
      "Model: ArchitRastogi/xgboost_sentence_splitting\n",
      "Type: xgboost\n",
      "================================================================================\n",
      "Loading embedding model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472954ea202648c5b6e5c92028df72ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10422da750ac4215a60091a63b5549e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b04eaf1eb2b46a2a8a1b4286c901956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf9df37773544c097b5ce5e6e94d33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afeb078071934013be61ce406abd9f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d363afcb67a43698d2023f64a5dc4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f68839ba1494d9ca5f5894d7779eea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0677d3aca9d242789cfdbbe481c0fa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65801e7ca1944e15842c34f5a371e9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ea5a9fc07744f9a2a05141c6db3c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded\n",
      "Downloading XGBoost model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3b29410a07453891f82c2188420906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "xgboost_model.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model loaded\n",
      "Creating features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf9ca3558414778a1199ec1a61d1595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating XGBoost features:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (1508, 394)\n",
      "Running inference...\n",
      "\n",
      "Results:\n",
      "  Accuracy:  0.9940\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.8977\n",
      "  F1 Score:  0.9461\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Split (0)     0.9937    1.0000    0.9968      1420\n",
      "   Split (1)     1.0000    0.8977    0.9461        88\n",
      "\n",
      "    accuracy                         0.9940      1508\n",
      "   macro avg     0.9969    0.9489    0.9715      1508\n",
      "weighted avg     0.9941    0.9940    0.9939      1508\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1420    0]\n",
      " [   9   79]]\n",
      "Classification report saved: inference_output/xgboost_classification_report.txt\n",
      "\n",
      "All models evaluated.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model_predictions = {}\n",
    "\n",
    "for model_name, model_config in MODELS.items():\n",
    "    result = test_model(model_name, model_config, sentences, labels)\n",
    "    if result:\n",
    "        predictions = result.pop('predictions')\n",
    "        classification_report_text = result.pop('classification_report')\n",
    "        results.append(result)\n",
    "        model_predictions[model_name] = predictions\n",
    "        \n",
    "        # Save individual classification report\n",
    "        report_file = os.path.join(OUTPUT_DIR, f\"{model_name}_classification_report.txt\")\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(f\"Model: {model_name}\\n\")\n",
    "            f.write(f\"HuggingFace: {result['huggingface_link']}\\n\\n\")\n",
    "            f.write(f\"Metrics:\\n\")\n",
    "            f.write(f\"  Accuracy:  {result['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"  Precision: {result['precision']:.4f}\\n\")\n",
    "            f.write(f\"  Recall:    {result['recall']:.4f}\\n\")\n",
    "            f.write(f\"  F1 Score:  {result['f1']:.4f}\\n\\n\")\n",
    "            f.write(f\"Classification Report:\\n{classification_report_text}\\n\")\n",
    "        print(f\"Classification report saved: {report_file}\")\n",
    "\n",
    "print(\"\\nAll models evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "       model_name  model_type  accuracy  precision   recall       f1\n",
      "          xgboost     xgboost  0.994032   1.000000 0.897727 0.946108\n",
      "  xlm-roberta-crf transformer  0.982095   0.769912 0.988636 0.865672\n",
      "bert-italian-base transformer  0.962892   0.663043 0.693182 0.677778\n",
      " bert-italian-crf transformer  0.955214   0.593750 0.647727 0.619565\n",
      "\n",
      "Summary saved to: inference_output/evaluation_summary_20251214_170939.csv\n",
      "\n",
      "Best Performing Model:\n",
      "  Model: xgboost\n",
      "  F1 Score: 0.9461\n",
      "  Accuracy: 0.9940\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.8977\n",
      "  Link: https://huggingface.co/ArchitRastogi/xgboost_sentence_splitting\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('f1', ascending=False)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(results_df[['model_name', 'model_type', 'accuracy', 'precision', 'recall', 'f1']].to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Save summary CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    summary_file = os.path.join(OUTPUT_DIR, f\"evaluation_summary_{timestamp}.csv\")\n",
    "    results_df.to_csv(summary_file, index=False)\n",
    "    print(f\"Summary saved to: {summary_file}\")\n",
    "    \n",
    "    # Display best model\n",
    "    print(\"\\nBest Performing Model:\")\n",
    "    best_model = results_df.iloc[0]\n",
    "    print(f\"  Model: {best_model['model_name']}\")\n",
    "    print(f\"  F1 Score: {best_model['f1']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_model['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_model['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_model['recall']:.4f}\")\n",
    "    print(f\"  Link: {best_model['huggingface_link']}\")\n",
    "else:\n",
    "    print(\"No results generated. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Prediction Output Files\n",
    "\n",
    "Creates CSV files with predictions for each model in the same format as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction output files...\n",
      "\n",
      "Saved submission file: inference_output/exACSAI-hw2_split-bert-italian-crf.csv\n",
      "Saved submission file: inference_output/exACSAI-hw2_split-xlm-roberta-crf.csv\n",
      "Saved submission file: inference_output/exACSAI-hw2_split-bert-italian-base.csv\n",
      "Saved submission file: inference_output/exACSAI-hw2_split-xgboost.csv\n",
      "\n",
      "All prediction files generated successfully.\n",
      "\n",
      "Prediction files :\n",
      "  - exACSAI-hw2_split-bert-italian-crf.csv\n",
      "  - exACSAI-hw2_split-xlm-roberta-crf.csv\n",
      "  - exACSAI-hw2_split-bert-italian-base.csv\n",
      "  - exACSAI-hw2_split-xgboost.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating prediction output files...\\n\")\n",
    "\n",
    "\n",
    "for model_name, predictions in model_predictions.items():\n",
    "    # Predictions are longer due to stride creating overlapping chunks\n",
    "    # We need to map predictions back to original tokens\n",
    "    \n",
    "    # Reconstruct predictions to match original token count\n",
    "    pred_idx = 0\n",
    "    aligned_predictions = []\n",
    "    \n",
    "    for sent_idx, (sent, labs) in enumerate(zip(sentences, labels)):\n",
    "        sent_length = len(sent)\n",
    "        # Take only the number of predictions matching this sentence length\n",
    "        if pred_idx + sent_length <= len(predictions):\n",
    "            aligned_predictions.extend(predictions[pred_idx:pred_idx + sent_length])\n",
    "            pred_idx += sent_length\n",
    "        else:\n",
    "            # Handle edge case: not enough predictions\n",
    "            aligned_predictions.extend(predictions[pred_idx:])\n",
    "            break\n",
    "    \n",
    "    # Ensure we have exactly the right number of predictions\n",
    "    if len(aligned_predictions) != len(raw_df):\n",
    "        print(f\"Warning: {model_name} prediction length mismatch. Expected {len(raw_df)}, got {len(aligned_predictions)}\")\n",
    "        # Pad or truncate to match\n",
    "        if len(aligned_predictions) < len(raw_df):\n",
    "            aligned_predictions.extend([0] * (len(raw_df) - len(aligned_predictions)))\n",
    "        else:\n",
    "            aligned_predictions = aligned_predictions[:len(raw_df)]\n",
    "    \n",
    "    # Create output dataframe with required format\n",
    "    output_df = pd.DataFrame({\n",
    "        'token': raw_df['token'],\n",
    "        'label': aligned_predictions  # Use 'label' as column name per submission requirements\n",
    "    })\n",
    "    \n",
    "    # Save with submission naming convention: \"groupname-hw2_split-modelname.csv\"\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"{GROUP_NAME}-hw2_split-{model_name}.csv\")\n",
    "    output_df.to_csv(output_file, sep=',', index=False)  # Comma separated, no index\n",
    "    print(f\"Saved submission file: {output_file}\")\n",
    "    \n",
    "    # # Also save comparison file with original labels (for your reference)\n",
    "    # comparison_df = pd.DataFrame({\n",
    "    #     'token': raw_df['token'],\n",
    "    #     'true_label': raw_df['label'],\n",
    "    #     'predicted_label': aligned_predictions\n",
    "    # })\n",
    "    # comparison_file = os.path.join(OUTPUT_DIR, f\"{model_name}_comparison.csv\")\n",
    "    # comparison_df.to_csv(comparison_file, sep=',', index=False)\n",
    "    # print(f\"Saved comparison file: {comparison_file}\")\n",
    "\n",
    "print(f\"\\nAll prediction files generated successfully.\")\n",
    "print(f\"\\nPrediction files :\")\n",
    "for model_name in model_predictions.keys():\n",
    "    print(f\"  - {GROUP_NAME}-hw2_split-{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "The inference pipeline has completed. The following outputs are available in the `inference_output` directory:\n",
    "\n",
    "1. **Evaluation Summary**: CSV file with metrics for all models\n",
    "2. **Classification Reports**: Individual text files with detailed metrics per model\n",
    "3. **Prediction Files**: CSV files with predicted labels for each model\n",
    "4. **Comparison Files**: CSV files with original labels and predictions side-by-side\n",
    "\n",
    "The models are ranked by F1 score, with the best performing model identified above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
