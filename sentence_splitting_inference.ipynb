{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Sentence Boundary Detection - Inference Pipeline\n",
    "\n",
    "This notebook provides an inference pipeline for sentence boundary detection in Italian text using fine-tuned encoder models with and without CRF layers.\n",
    "\n",
    "## Models\n",
    "\n",
    "The following models are evaluated:\n",
    "\n",
    "1. **BERT-Italian-CRF**: dbmdz/bert-base-italian-xxl-cased with CRF layer\n",
    "2. **XLM-RoBERTa-CRF**: FacebookAI/xlm-roberta-base with CRF layer  \n",
    "3. **BERT-Italian-Base**: dbmdz/bert-base-italian-xxl-cased (standard encoder)\n",
    "\n",
    "## Task\n",
    "\n",
    "Binary token classification:\n",
    "- Label 0: Token does not end a sentence\n",
    "- Label 1: Token ends a sentence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers pytorch-crf scikit-learn pandas numpy tqdm huggingface-hub safetensors protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n",
      "Number of models to evaluate: 3\n",
      "Output directory: inference_output\n"
     ]
    }
   ],
   "source": [
    "# Configuration Cell\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"bert-italian-crf\": {\n",
    "        \"model_path\": \"ArchitRastogi/bert-base-italian-xxl-cased-sentence-splitter-CRF\",\n",
    "        \"base_model\": \"dbmdz/bert-base-italian-xxl-cased\",\n",
    "        \"is_crf\": True\n",
    "    },\n",
    "    \"xlm-roberta-crf\": {\n",
    "        \"model_path\": \"ArchitRastogi/xlm-roberta-base-italian-sentence-splitter-CRF\",\n",
    "        \"base_model\": \"FacebookAI/xlm-roberta-base\",\n",
    "        \"is_crf\": True\n",
    "    },\n",
    "    \"bert-italian-base\": {\n",
    "        \"model_path\": \"ArchitRastogi/bert-base-italian-xxl-cased-sentence-splitter-base\",\n",
    "        \"base_model\": \"dbmdz/bert-base-italian-xxl-cased\",\n",
    "        \"is_crf\": False\n",
    "    },\n",
    "}\n",
    "\n",
    "# Inference parameters\n",
    "MAX_LENGTH = 512\n",
    "STRIDE = 64\n",
    "BATCH_SIZE = 16\n",
    "CACHE_DIR = \"cache\"\n",
    "OUTPUT_DIR = \"inference_output\"\n",
    "\n",
    "# Input file configuration\n",
    "# Set to None to use default OOD_test.csv, or provide path to custom CSV\n",
    "CUSTOM_INPUT_FILE = None  # e.g., \"path/to/your/file.csv\"\n",
    "DEFAULT_INPUT_FILE = \"OOD_test.csv\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Number of models to evaluate: {len(MODELS)}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA RTX A4500\n",
      "Memory: 21.15 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, \n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model Architecture\n",
    "\n",
    "The CRF wrapper adds a Conditional Random Field layer on top of the encoder for structured prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined.\n"
     ]
    }
   ],
   "source": [
    "class BERTWithCRF(nn.Module):\n",
    "    \"\"\"BERT encoder with CRF layer for token classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.classifier(sequence_output)\n",
    "        \n",
    "        mask = attention_mask.bool()\n",
    "        \n",
    "        if labels is not None:\n",
    "            labels_crf = labels.clone()\n",
    "            valid_labels_mask = (labels != -100)\n",
    "            labels_crf[labels_crf == -100] = 0\n",
    "            \n",
    "            log_likelihood = self.crf(emissions, labels_crf, mask=mask, reduction='none')\n",
    "            \n",
    "            batch_size = input_ids.size(0)\n",
    "            masked_log_likelihood = []\n",
    "            for i in range(batch_size):\n",
    "                n_valid = valid_labels_mask[i].sum()\n",
    "                if n_valid > 0:\n",
    "                    masked_log_likelihood.append(log_likelihood[i])\n",
    "            \n",
    "            if len(masked_log_likelihood) > 0:\n",
    "                loss = -torch.stack(masked_log_likelihood).mean()\n",
    "            else:\n",
    "                loss = -log_likelihood.mean()\n",
    "            \n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return {'loss': loss, 'logits': emissions, 'predictions': predictions}\n",
    "        else:\n",
    "            predictions = self.crf.decode(emissions, mask=mask)\n",
    "            return {'logits': emissions, 'predictions': predictions}\n",
    "\n",
    "print(\"Model architecture defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading Functions\n",
    "\n",
    "Expected CSV format (semicolon-separated):\n",
    "```\n",
    "token;label\n",
    "C';0\n",
    "era;0\n",
    "una;0\n",
    "volta;0\n",
    ".;1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined.\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(filepath):\n",
    "    \"\"\"Load test data from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file with token;label format\n",
    "        \n",
    "    Returns:\n",
    "        sentences: List of token lists\n",
    "        labels: List of label lists\n",
    "        df: Raw dataframe\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {filepath}\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        \n",
    "        # Skip header if present\n",
    "        if first_line.lower().startswith('pinocchio') or first_line == 'token;label':\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split(';')\n",
    "                if len(parts) == 2:\n",
    "                    token, label = parts\n",
    "                    if token == 'token' and label == 'label':\n",
    "                        continue\n",
    "                    try:\n",
    "                        data.append([token, int(label)])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        else:\n",
    "            # First line is data\n",
    "            parts = first_line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                token, label = parts\n",
    "                try:\n",
    "                    data.append([token, int(label)])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split(';')\n",
    "                if len(parts) == 2:\n",
    "                    token, label = parts\n",
    "                    try:\n",
    "                        data.append([token, int(label)])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['token', 'label'])\n",
    "    \n",
    "    # Group into sentences (split at label=1)\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_sent = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        token = str(row['token'])\n",
    "        label = int(row['label'])\n",
    "        current_sent.append(token)\n",
    "        current_labels.append(label)\n",
    "        if label == 1:\n",
    "            sentences.append(current_sent)\n",
    "            labels.append(current_labels)\n",
    "            current_sent = []\n",
    "            current_labels = []\n",
    "    \n",
    "    if current_sent:\n",
    "        sentences.append(current_sent)\n",
    "        labels.append(current_labels)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} tokens grouped into {len(sentences)} sentences\")\n",
    "    print(f\"Label distribution:\\n{df['label'].value_counts().sort_index()}\")\n",
    "    \n",
    "    return sentences, labels, df\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined.\n"
     ]
    }
   ],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    \"\"\"Dataset for inference with stride support.\"\"\"\n",
    "    \n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512, stride=64):\n",
    "        self.encodings = []\n",
    "        self.labels_aligned = []\n",
    "        self.original_indices = []\n",
    "        \n",
    "        for idx, (sent, labs) in enumerate(tqdm(zip(sentences, labels), \n",
    "                                                 desc=\"Tokenizing\", \n",
    "                                                 total=len(sentences))):\n",
    "            text = ' '.join(sent)\n",
    "            \n",
    "            encoding = tokenizer(\n",
    "                text, \n",
    "                truncation=True, \n",
    "                max_length=max_length,\n",
    "                stride=stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "            \n",
    "            for i in range(len(encoding['input_ids'])):\n",
    "                self.encodings.append({\n",
    "                    'input_ids': encoding['input_ids'][i],\n",
    "                    'attention_mask': encoding['attention_mask'][i]\n",
    "                })\n",
    "                \n",
    "                word_ids = encoding.word_ids(batch_index=i)\n",
    "                label_ids = []\n",
    "                previous_word_idx = None\n",
    "                \n",
    "                for word_idx in word_ids:\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:\n",
    "                        label_ids.append(labs[word_idx] if word_idx < len(labs) else 0)\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                \n",
    "                self.labels_aligned.append(label_ids)\n",
    "                self.original_indices.append(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        item['labels'] = torch.tensor(self.labels_aligned[idx])\n",
    "        item['idx'] = self.original_indices[idx]\n",
    "        return item\n",
    "\n",
    "print(\"Dataset class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference functions defined.\n"
     ]
    }
   ],
   "source": [
    "def run_inference(model, dataloader, is_crf=False):\n",
    "    \"\"\"Run inference on dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        dataloader: DataLoader instance\n",
    "        is_crf: Whether model uses CRF layer\n",
    "        \n",
    "    Returns:\n",
    "        predictions: Numpy array of predictions\n",
    "        labels: Numpy array of true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Running inference\"):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            if is_crf:\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                predictions = outputs['predictions']\n",
    "                \n",
    "                for i, pred_seq in enumerate(predictions):\n",
    "                    mask = (labels[i] != -100).cpu().numpy()\n",
    "                    pred_seq_padded = pred_seq + [0] * (len(mask) - len(pred_seq))\n",
    "                    pred_masked = np.array(pred_seq_padded)[mask]\n",
    "                    label_masked = labels[i].cpu().numpy()[mask]\n",
    "                    all_predictions.extend(pred_masked.tolist())\n",
    "                    all_labels.extend(label_masked.tolist())\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                for i in range(predictions.shape[0]):\n",
    "                    mask = (labels[i] != -100).cpu().numpy()\n",
    "                    pred_masked = predictions[i].cpu().numpy()[mask]\n",
    "                    label_masked = labels[i].cpu().numpy()[mask]\n",
    "                    all_predictions.extend(pred_masked.tolist())\n",
    "                    all_labels.extend(label_masked.tolist())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    \"\"\"Compute evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted labels\n",
    "        labels: True labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    class_report = classification_report(\n",
    "        labels, predictions, \n",
    "        target_names=['No Split (0)', 'Split (1)'],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "print(\"Inference functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Loading and Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model testing function defined.\n"
     ]
    }
   ],
   "source": [
    "def test_model(model_name, model_config, sentences, labels):\n",
    "    \"\"\"Load and test a single model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name identifier for the model\n",
    "        model_config: Configuration dictionary\n",
    "        sentences: List of sentence token lists\n",
    "        labels: List of label lists\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of results or None if failed\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {model_name}\")\n",
    "    print(f\"Model: {model_config['model_path']}\")\n",
    "    print(f\"Type: {'CRF' if model_config['is_crf'] else 'Base Encoder'}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_config['base_model'],\n",
    "            cache_dir=CACHE_DIR,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"Tokenizer loaded: {model_config['base_model']}\")\n",
    "        \n",
    "        # Load model\n",
    "        if model_config['is_crf']:\n",
    "            from huggingface_hub import hf_hub_download\n",
    "            from safetensors.torch import load_file\n",
    "            \n",
    "            # Load base encoder\n",
    "            base_encoder = AutoModel.from_pretrained(\n",
    "                model_config['base_model'],\n",
    "                cache_dir=CACHE_DIR,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"Base encoder loaded\")\n",
    "            \n",
    "            # Create CRF wrapper\n",
    "            model = BERTWithCRF(base_encoder, num_labels=2)\n",
    "            print(\"CRF wrapper created\")\n",
    "            \n",
    "            # Load weights\n",
    "            try:\n",
    "                model_file = hf_hub_download(\n",
    "                    repo_id=model_config['model_path'],\n",
    "                    filename=\"model.safetensors\",\n",
    "                    cache_dir=CACHE_DIR\n",
    "                )\n",
    "                state_dict = load_file(model_file)\n",
    "                model.load_state_dict(state_dict, strict=False)\n",
    "                print(\"Weights loaded from safetensors\")\n",
    "            except:\n",
    "                model_file = hf_hub_download(\n",
    "                    repo_id=model_config['model_path'],\n",
    "                    filename=\"pytorch_model.bin\",\n",
    "                    cache_dir=CACHE_DIR\n",
    "                )\n",
    "                state_dict = torch.load(model_file, map_location=DEVICE)\n",
    "                model.load_state_dict(state_dict, strict=False)\n",
    "                print(\"Weights loaded from pytorch_model.bin\")\n",
    "        else:\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                model_config['model_path'],\n",
    "                cache_dir=CACHE_DIR,\n",
    "                num_labels=2,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"Model loaded directly\")\n",
    "        \n",
    "        model = model.to(DEVICE)\n",
    "        model.eval()\n",
    "        print(f\"Model moved to {DEVICE}\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        test_dataset = InferenceDataset(sentences, labels, tokenizer, MAX_LENGTH, STRIDE)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Run inference\n",
    "        predictions, true_labels = run_inference(model, test_loader, is_crf=model_config['is_crf'])\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_metrics(predictions, true_labels)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {metrics['f1']:.4f}\")\n",
    "        print(f\"\\nClassification Report:\\n{metrics['classification_report']}\")\n",
    "        print(f\"\\nConfusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, tokenizer, test_dataset, test_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'model_path': model_config['model_path'],\n",
    "            'model_type': 'CRF' if model_config['is_crf'] else 'Base',\n",
    "            'huggingface_link': f\"https://huggingface.co/{model_config['model_path']}\",\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1': metrics['f1'],\n",
    "            'predictions': predictions,\n",
    "            'classification_report': metrics['classification_report']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing model {model_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"Model testing function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Input Data\n",
    "\n",
    "Loads either the custom input file or the default OOD test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: OOD_test.csv\n",
      "\n",
      "Loading data from: OOD_test.csv\n",
      "Loaded 1508 tokens grouped into 88 sentences\n",
      "Label distribution:\n",
      "label\n",
      "0    1420\n",
      "1      88\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset loaded successfully.\n",
      "Total tokens: 1508\n",
      "Total sentences: 88\n"
     ]
    }
   ],
   "source": [
    "# Determine input file\n",
    "input_file = CUSTOM_INPUT_FILE if CUSTOM_INPUT_FILE else DEFAULT_INPUT_FILE\n",
    "\n",
    "print(f\"Input file: {input_file}\")\n",
    "print()\n",
    "\n",
    "# Load data\n",
    "sentences, labels, raw_df = load_test_data(input_file)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully.\")\n",
    "print(f\"Total tokens: {len(raw_df)}\")\n",
    "print(f\"Total sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Inference on All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing: bert-italian-crf\n",
      "Model: ArchitRastogi/bert-base-italian-xxl-cased-sentence-splitter-CRF\n",
      "Type: CRF\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c736c539df0a4ddc8ea5451061f98760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3b0773fbe34d5f8b97d690cda2b43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9937d578c25749d780e8b226aa439d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: dbmdz/bert-base-italian-xxl-cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5969aa42ecfe49b48c2b7bf1ceed38dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base encoder loaded\n",
      "CRF wrapper created\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e3e4d880ff4b2ba06ddc2e6dce30b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded from safetensors\n",
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4459001a62374d5dae987746f0931315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ca2da6921445fa994c183bb2e1716e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy:  0.9552\n",
      "  Precision: 0.5938\n",
      "  Recall:    0.6477\n",
      "  F1 Score:  0.6196\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Split (0)     0.9789    0.9736    0.9762      1475\n",
      "   Split (1)     0.5938    0.6477    0.6196        88\n",
      "\n",
      "    accuracy                         0.9552      1563\n",
      "   macro avg     0.7863    0.8106    0.7979      1563\n",
      "weighted avg     0.9572    0.9552    0.9561      1563\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1436   39]\n",
      " [  31   57]]\n",
      "Classification report saved: inference_output/bert-italian-crf_classification_report.txt\n",
      "\n",
      "================================================================================\n",
      "Testing: xlm-roberta-crf\n",
      "Model: ArchitRastogi/xlm-roberta-base-italian-sentence-splitter-CRF\n",
      "Type: CRF\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a30147588a425a8d3806242d87da93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebbb3918d134eb9ae8b3ea824b0ba9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e04403affc74380bb7124080292bb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2954efc5eb439ba8350201d7468bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: FacebookAI/xlm-roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e192eb8bbb4085ac675ff69357c276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base encoder loaded\n",
      "CRF wrapper created\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82e2def0841412389e637ca19f1708c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded from safetensors\n",
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6651547a03c04ed684e5c5256b83366e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f6272e9fde438a9680ba64431c8826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy:  0.9821\n",
      "  Precision: 0.7699\n",
      "  Recall:    0.9886\n",
      "  F1 Score:  0.8657\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Split (0)     0.9993    0.9817    0.9904      1420\n",
      "   Split (1)     0.7699    0.9886    0.8657        88\n",
      "\n",
      "    accuracy                         0.9821      1508\n",
      "   macro avg     0.8846    0.9852    0.9280      1508\n",
      "weighted avg     0.9859    0.9821    0.9831      1508\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1394   26]\n",
      " [   1   87]]\n",
      "Classification report saved: inference_output/xlm-roberta-crf_classification_report.txt\n",
      "\n",
      "================================================================================\n",
      "Testing: bert-italian-base\n",
      "Model: ArchitRastogi/bert-base-italian-xxl-cased-sentence-splitter-base\n",
      "Type: Base Encoder\n",
      "================================================================================\n",
      "Tokenizer loaded: dbmdz/bert-base-italian-xxl-cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dae2e6aa13a44f1b86f866ea63df015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab861509bdcc4b8baadb57e2878060fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded directly\n",
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed63ae77d7a742c18474c3b7ce70d791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f20f2578d4045b0bb2c0ccaaef72c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy:  0.9629\n",
      "  Precision: 0.6630\n",
      "  Recall:    0.6932\n",
      "  F1 Score:  0.6778\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Split (0)     0.9816    0.9790    0.9803      1475\n",
      "   Split (1)     0.6630    0.6932    0.6778        88\n",
      "\n",
      "    accuracy                         0.9629      1563\n",
      "   macro avg     0.8223    0.8361    0.8290      1563\n",
      "weighted avg     0.9637    0.9629    0.9633      1563\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1444   31]\n",
      " [  27   61]]\n",
      "Classification report saved: inference_output/bert-italian-base_classification_report.txt\n",
      "\n",
      "All models evaluated.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model_predictions = {}\n",
    "\n",
    "for model_name, model_config in MODELS.items():\n",
    "    result = test_model(model_name, model_config, sentences, labels)\n",
    "    if result:\n",
    "        predictions = result.pop('predictions')\n",
    "        classification_report_text = result.pop('classification_report')\n",
    "        results.append(result)\n",
    "        model_predictions[model_name] = predictions\n",
    "        \n",
    "        # Save individual classification report\n",
    "        report_file = os.path.join(OUTPUT_DIR, f\"{model_name}_classification_report.txt\")\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(f\"Model: {model_name}\\n\")\n",
    "            f.write(f\"HuggingFace: {result['huggingface_link']}\\n\\n\")\n",
    "            f.write(f\"Metrics:\\n\")\n",
    "            f.write(f\"  Accuracy:  {result['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"  Precision: {result['precision']:.4f}\\n\")\n",
    "            f.write(f\"  Recall:    {result['recall']:.4f}\\n\")\n",
    "            f.write(f\"  F1 Score:  {result['f1']:.4f}\\n\\n\")\n",
    "            f.write(f\"Classification Report:\\n{classification_report_text}\\n\")\n",
    "        print(f\"Classification report saved: {report_file}\")\n",
    "\n",
    "print(\"\\nAll models evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "       model_name model_type  accuracy  precision   recall       f1\n",
      "  xlm-roberta-crf        CRF  0.982095   0.769912 0.988636 0.865672\n",
      "bert-italian-base       Base  0.962892   0.663043 0.693182 0.677778\n",
      " bert-italian-crf        CRF  0.955214   0.593750 0.647727 0.619565\n",
      "\n",
      "Summary saved to: inference_output/evaluation_summary_20251214_154526.csv\n",
      "\n",
      "Best Performing Model:\n",
      "  Model: xlm-roberta-crf\n",
      "  F1 Score: 0.8657\n",
      "  Accuracy: 0.9821\n",
      "  Precision: 0.7699\n",
      "  Recall: 0.9886\n",
      "  Link: https://huggingface.co/ArchitRastogi/xlm-roberta-base-italian-sentence-splitter-CRF\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('f1', ascending=False)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(results_df[['model_name', 'model_type', 'accuracy', 'precision', 'recall', 'f1']].to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Save summary CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    summary_file = os.path.join(OUTPUT_DIR, f\"evaluation_summary_{timestamp}.csv\")\n",
    "    results_df.to_csv(summary_file, index=False)\n",
    "    print(f\"Summary saved to: {summary_file}\")\n",
    "    \n",
    "    # Display best model\n",
    "    print(\"\\nBest Performing Model:\")\n",
    "    best_model = results_df.iloc[0]\n",
    "    print(f\"  Model: {best_model['model_name']}\")\n",
    "    print(f\"  F1 Score: {best_model['f1']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_model['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_model['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_model['recall']:.4f}\")\n",
    "    print(f\"  Link: {best_model['huggingface_link']}\")\n",
    "else:\n",
    "    print(\"No results generated. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Prediction Output Files\n",
    "\n",
    "Creates CSV files with predictions for each model in the same format as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction output files...\n",
      "\n",
      "Saved: inference_output/bert-italian-crf_predictions.csv\n",
      "Saved: inference_output/bert-italian-crf_comparison.csv\n",
      "Saved: inference_output/xlm-roberta-crf_predictions.csv\n",
      "Saved: inference_output/xlm-roberta-crf_comparison.csv\n",
      "Saved: inference_output/bert-italian-base_predictions.csv\n",
      "Saved: inference_output/bert-italian-base_comparison.csv\n",
      "\n",
      "All prediction files generated successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating prediction output files...\\n\")\n",
    "\n",
    "for model_name, predictions in model_predictions.items():\n",
    "    # Predictions are longer due to stride creating overlapping chunks\n",
    "    # We need to map predictions back to original tokens\n",
    "    \n",
    "    # Reconstruct predictions to match original token count\n",
    "    pred_idx = 0\n",
    "    aligned_predictions = []\n",
    "    \n",
    "    for sent_idx, (sent, labs) in enumerate(zip(sentences, labels)):\n",
    "        sent_length = len(sent)\n",
    "        # Take only the number of predictions matching this sentence length\n",
    "        if pred_idx + sent_length <= len(predictions):\n",
    "            aligned_predictions.extend(predictions[pred_idx:pred_idx + sent_length])\n",
    "            pred_idx += sent_length\n",
    "        else:\n",
    "            # Handle edge case: not enough predictions\n",
    "            aligned_predictions.extend(predictions[pred_idx:])\n",
    "            break\n",
    "    \n",
    "    # Ensure we have exactly the right number of predictions\n",
    "    if len(aligned_predictions) != len(raw_df):\n",
    "        print(f\"Warning: {model_name} prediction length mismatch. Expected {len(raw_df)}, got {len(aligned_predictions)}\")\n",
    "        # Pad or truncate to match\n",
    "        if len(aligned_predictions) < len(raw_df):\n",
    "            aligned_predictions.extend([0] * (len(raw_df) - len(aligned_predictions)))\n",
    "        else:\n",
    "            aligned_predictions = aligned_predictions[:len(raw_df)]\n",
    "    \n",
    "    # Create output dataframe\n",
    "    output_df = raw_df.copy()\n",
    "    output_df['prediction'] = aligned_predictions\n",
    "    \n",
    "    # Save with predictions\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"{model_name}_predictions.csv\")\n",
    "    output_df[['token', 'prediction']].to_csv(output_file, sep=';', index=False)\n",
    "    print(f\"Saved: {output_file}\")\n",
    "    \n",
    "    # Also save with original labels for comparison\n",
    "    comparison_file = os.path.join(OUTPUT_DIR, f\"{model_name}_comparison.csv\")\n",
    "    output_df[['token', 'label', 'prediction']].to_csv(comparison_file, sep=';', index=False)\n",
    "    print(f\"Saved: {comparison_file}\")\n",
    "\n",
    "print(\"\\nAll prediction files generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "The inference pipeline has completed. The following outputs are available in the `inference_output` directory:\n",
    "\n",
    "1. **Evaluation Summary**: CSV file with metrics for all models\n",
    "2. **Classification Reports**: Individual text files with detailed metrics per model\n",
    "3. **Prediction Files**: CSV files with predicted labels for each model\n",
    "4. **Comparison Files**: CSV files with original labels and predictions side-by-side\n",
    "\n",
    "The models are ranked by F1 score, with the best performing model identified above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
