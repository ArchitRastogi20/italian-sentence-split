root@da7e5d60e8de:/# python train_encoders.py
wandb: Currently logged in as: architk2003 (architk2003-sapienza-universit-di-roma) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
================================================================================
LOADING DATA
================================================================================
Loading manzoni_train_tokens.csv...
  Loaded 74764 tokens
  Grouped into 2447 sentences
Loading manzoni_dev_tokens.csv...
  Loaded 9343 tokens
  Grouped into 324 sentences
Loading OOD_test.csv...
  Loaded 1508 tokens
  Grouped into 88 sentences

Train: 2447 sentences
Dev: 324 sentences
OOD: 88 sentences

================================================================================
TRAINING: xlm-roberta-base
================================================================================
Preparing datasets...
Tokenizing: 100%|████████████████████████████████████████████████████████████████████████████████████| 2447/2447 [00:01<00:00, 1598.90it/s]
Tokenizing: 100%|██████████████████████████████████████████████████████████████████████████████████████| 324/324 [00:00<00:00, 1647.37it/s]
Tokenizing: 100%|████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 1811.58it/s]

Hyperparameter tuning for xlm-roberta-base...
[I 2025-11-09 14:23:15,347] A new study created in memory with name: xlm-roberta-base_study
  0%|                                                                                                               | 0/20 [00:00<?, ?it/s]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.5507, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.03}                                                                    
{'eval_loss': 0.051908910274505615, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 2.0693, 'eval_samples_per_second': 156.575, 'eval_steps_per_second': 5.316, 'epoch': 1.0}                                         
{'loss': 0.2316, 'grad_norm': 0.437549889087677, 'learning_rate': 2.0241280349051328e-05, 'epoch': 1.29}                                   
{'eval_loss': 0.00017963172285817564, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.3858, 'eval_samples_per_second': 135.802, 'eval_steps_per_second': 4.611, 'epoch': 2.0}                                                      
{'loss': 0.0052, 'grad_norm': 0.025079265236854553, 'learning_rate': 3.764102041409854e-05, 'epoch': 2.58}                                 
{'eval_loss': 3.642432056949474e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1396, 'eval_samples_per_second': 151.431, 'eval_steps_per_second': 5.141, 'epoch': 3.0}                                                       
{'loss': 0.0005, 'grad_norm': 0.0693850964307785, 'learning_rate': 3.11734911333256e-05, 'epoch': 3.86}                                    
{'eval_loss': 0.0005154889076948166, 'eval_precision': 0.9969230769230769, 'eval_recall': 1.0, 'eval_f1': 0.9984591679506933, 'eval_accuracy': 0.9998929679974312, 'eval_runtime': 2.0802, 'eval_samples_per_second': 155.755, 'eval_steps_per_second': 5.288, 'epoch': 4.0}          
{'eval_loss': 2.1480807845364325e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1067, 'eval_samples_per_second': 153.797, 'eval_steps_per_second': 5.221, 'epoch': 5.0}                                                      
{'train_runtime': 248.6229, 'train_samples_per_second': 98.422, 'train_steps_per_second': 1.569, 'train_loss': 0.06260885520814322, 'epoch': 5.0}                                                                                                                                     
 50%|██████████████████████████████████████████████████                                                  | 195/390 [04:08<04:08,  1.28s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 12.51it/s]
[I 2025-11-09 14:27:33,250] Trial 0 finished with value: 1.0 and parameters: {'lr': 3.8417123927791296e-05, 'weight_decay': 0.07435023736314793, 'warmup_ratio': 0.23647663064662078, 'batch_size': 16}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:   5%|███▌                                                                  | 1/20 [04:17<1:21:40, 257.91s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6153, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.05}                                                                    
{'eval_loss': 0.036686718463897705, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 1.8501, 'eval_samples_per_second': 175.126, 'eval_steps_per_second': 3.243, 'epoch': 1.0}                                         
{'eval_loss': 0.0005061636329628527, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5503, 'eval_samples_per_second': 127.044, 'eval_steps_per_second': 2.353, 'epoch': 2.0}                                                       
{'loss': 0.0739, 'grad_norm': 0.02316942811012268, 'learning_rate': 1.869136891082632e-05, 'epoch': 2.52}                                  
{'eval_loss': 8.003727270988747e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1499, 'eval_samples_per_second': 150.703, 'eval_steps_per_second': 2.791, 'epoch': 3.0}                                                       
{'eval_loss': 5.018959564040415e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 1.9429, 'eval_samples_per_second': 166.759, 'eval_steps_per_second': 3.088, 'epoch': 4.0}                                                       
{'loss': 0.0005, 'grad_norm': 0.004592180252075195, 'learning_rate': 1.2502173907241445e-05, 'epoch': 5.0}                                 
{'eval_loss': 4.255425665178336e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5874, 'eval_samples_per_second': 125.221, 'eval_steps_per_second': 2.319, 'epoch': 5.0}                                                       
{'train_runtime': 235.4261, 'train_samples_per_second': 103.939, 'train_steps_per_second': 0.85, 'train_loss': 0.042616935670375826, 'epoch': 5.0}                                                                                                                                    
 50%|██████████████████████████████████████████████████                                                  | 100/200 [03:55<03:55,  2.35s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.66it/s]
[I 2025-11-09 14:31:38,528] Trial 1 finished with value: 1.0 and parameters: {'lr': 2.46329961142678e-05, 'weight_decay': 0.00684656062616903, 'warmup_ratio': 0.0036978694306036507, 'batch_size': 32}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  10%|███████                                                               | 2/20 [08:23<1:15:08, 250.48s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6153, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.05}                                                                    
{'eval_loss': 0.0673723891377449, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 2.5808, 'eval_samples_per_second': 125.544, 'eval_steps_per_second': 2.325, 'epoch': 1.0}                                           
{'eval_loss': 0.0004354627162683755, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.0342, 'eval_samples_per_second': 159.279, 'eval_steps_per_second': 2.95, 'epoch': 2.0}                                                        
{'loss': 0.1374, 'grad_norm': 0.009407071396708488, 'learning_rate': 3.260760916961251e-05, 'epoch': 2.52}                                 
{'eval_loss': 4.3838441342813894e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.0725, 'eval_samples_per_second': 156.334, 'eval_steps_per_second': 2.895, 'epoch': 3.0}                                                      
{'eval_loss': 3.890813604812138e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6129, 'eval_samples_per_second': 124.001, 'eval_steps_per_second': 2.296, 'epoch': 4.0}                                                       
{'loss': 0.0006, 'grad_norm': 0.005783457774668932, 'learning_rate': 2.181038759027062e-05, 'epoch': 5.0}                                  
{'eval_loss': 3.4813678212231025e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5425, 'eval_samples_per_second': 127.436, 'eval_steps_per_second': 2.36, 'epoch': 5.0}                                                       
{'train_runtime': 224.7391, 'train_samples_per_second': 108.882, 'train_steps_per_second': 0.89, 'train_loss': 0.07377179542556406, 'epoch': 5.0}                                                                                                                                     
 50%|██████████████████████████████████████████████████                                                  | 100/200 [03:44<03:44,  2.25s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.71it/s]
[I 2025-11-09 14:35:31,769] Trial 2 finished with value: 1.0 and parameters: {'lr': 3.843810882245713e-05, 'weight_decay': 0.04181582883253109, 'warmup_ratio': 0.10782412030086018, 'batch_size': 32}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  15%|██████████▌                                                           | 3/20 [12:16<1:08:44, 242.61s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6153, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.05}                                                                    
{'eval_loss': 0.026472650468349457, 'eval_precision': 0.8895348837209303, 'eval_recall': 0.9444444444444444, 'eval_f1': 0.9161676646706587, 'eval_accuracy': 0.994006207856149, 'eval_runtime': 2.6557, 'eval_samples_per_second': 122.004, 'eval_steps_per_second': 2.259, 'epoch': 1.0}                                                                                                                                        
{'eval_loss': 4.9365444283466786e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.521, 'eval_samples_per_second': 128.519, 'eval_steps_per_second': 2.38, 'epoch': 2.0}                                                        
{'loss': 0.0843, 'grad_norm': 0.008313372731208801, 'learning_rate': 3.562492990129298e-05, 'epoch': 2.52}                                 
{'eval_loss': 2.6398367481306195e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.106, 'eval_samples_per_second': 153.85, 'eval_steps_per_second': 2.849, 'epoch': 3.0}                                                        
{'eval_loss': 2.131775727320928e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.4013, 'eval_samples_per_second': 134.925, 'eval_steps_per_second': 2.499, 'epoch': 4.0}                                                       
{'loss': 0.0003, 'grad_norm': 0.0024076756089925766, 'learning_rate': 2.382859549689133e-05, 'epoch': 5.0}                                 
{'eval_loss': 1.6622079783701338e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1772, 'eval_samples_per_second': 148.816, 'eval_steps_per_second': 2.756, 'epoch': 5.0}                                                      
{'train_runtime': 240.3734, 'train_samples_per_second': 101.8, 'train_steps_per_second': 0.832, 'train_loss': 0.04759180871769786, 'epoch': 5.0}                                                                                                                                      
 50%|██████████████████████████████████████████████████                                                  | 100/200 [04:00<04:00,  2.40s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.86it/s]
[I 2025-11-09 14:39:41,789] Trial 3 finished with value: 1.0 and parameters: {'lr': 4.5297924112902334e-05, 'weight_decay': 0.017349292195139834, 'warmup_ratio': 0.039861964514033156, 'batch_size': 32}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  20%|██████████████                                                        | 4/20 [16:26<1:05:28, 245.53s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6089, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.01}                                                                    
{'loss': 0.3997, 'grad_norm': 0.766955554485321, 'learning_rate': 6.611468948824332e-06, 'epoch': 0.65}                                    
{'eval_loss': 0.030573682859539986, 'eval_precision': 0.8787878787878788, 'eval_recall': 0.9845679012345679, 'eval_f1': 0.9286754002911208, 'eval_accuracy': 0.9947554318741304, 'eval_runtime': 1.9151, 'eval_samples_per_second': 169.182, 'eval_steps_per_second': 10.965, 'epoch': 1.0}                                                                                 
{'loss': 0.0324, 'grad_norm': inf, 'learning_rate': 1.3357865835379775e-05, 'epoch': 1.3}                                                                                             
{'loss': 0.0014, 'grad_norm': 0.13890309631824493, 'learning_rate': 2.0104262721935216e-05, 'epoch': 1.95}                                                                            
{'eval_loss': 0.00011814286699518561, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2101, 'eval_samples_per_second': 146.598, 'eval_steps_per_second': 9.502, 'epoch': 2.0}                                                                                                                                            
{'loss': 0.0006, 'grad_norm': 0.01304793730378151, 'learning_rate': 2.685065960849066e-05, 'epoch': 2.6}                                                                              
{'eval_loss': 3.8883070374140516e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2897, 'eval_samples_per_second': 141.501, 'eval_steps_per_second': 9.171, 'epoch': 3.0}                                                                                                                                            
{'loss': 0.0005, 'grad_norm': 0.0055358754470944405, 'learning_rate': 2.902188532207199e-05, 'epoch': 3.25}                                                                           
{'loss': 0.0001, 'grad_norm': 0.0022331844083964825, 'learning_rate': 2.6236675598264694e-05, 'epoch': 3.9}                                                                           
{'eval_loss': 1.6472729839733802e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2781, 'eval_samples_per_second': 142.222, 'eval_steps_per_second': 9.218, 'epoch': 4.0}                                                                                                                                            
{'loss': 0.0002, 'grad_norm': 0.0021585312206298113, 'learning_rate': 2.3451465874457402e-05, 'epoch': 4.55}                                                                          
{'eval_loss': 1.0516838301555254e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6521, 'eval_samples_per_second': 122.167, 'eval_steps_per_second': 7.918, 'epoch': 5.0}                                                                                                                                            
{'train_runtime': 336.4833, 'train_samples_per_second': 72.723, 'train_steps_per_second': 2.288, 'train_loss': 0.05702943176170206, 'epoch': 5.0}                                     
 50%|███████████████████████████████████████████████████████████████████████▌                                                                       | 385/770 [05:36<05:36,  1.14it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 21.59it/s]
[I 2025-11-09 14:45:29,912] Trial 4 finished with value: 1.0 and parameters: {'lr': 3.0358785989499488e-05, 'weight_decay': 0.06993172178507015, 'warmup_ratio': 0.291389261903302, 'batch_size': 8}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  25%|█████████████████▌                                                    | 5/20 [22:14<1:10:37, 282.53s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6153, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.05}                                                                    
{'eval_loss': 0.09618404507637024, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 2.1766, 'eval_samples_per_second': 148.859, 'eval_steps_per_second': 2.757, 'epoch': 1.0}                                                                                                                                
{'eval_loss': 0.008909955620765686, 'eval_precision': 0.9, 'eval_recall': 1.0, 'eval_f1': 0.9473684210526315, 'eval_accuracy': 0.9961468479075244, 'eval_runtime': 2.5219, 'eval_samples_per_second': 128.474, 'eval_steps_per_second': 2.379, 'epoch': 2.0}                                                                                                                
{'loss': 0.169, 'grad_norm': 0.056557852774858475, 'learning_rate': 2.9421893480518502e-05, 'epoch': 2.52}                                                                            
{'eval_loss': 5.914660505368374e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 1.9787, 'eval_samples_per_second': 163.746, 'eval_steps_per_second': 3.032, 'epoch': 3.0}                                                                                                                                             
{'eval_loss': 4.985205305274576e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5042, 'eval_samples_per_second': 129.381, 'eval_steps_per_second': 2.396, 'epoch': 4.0}                                                                                                                                             
{'loss': 0.0007, 'grad_norm': 0.009419042617082596, 'learning_rate': 1.9679544645909728e-05, 'epoch': 5.0}                                                                            
{'eval_loss': 4.078822166775353e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6202, 'eval_samples_per_second': 123.653, 'eval_steps_per_second': 2.29, 'epoch': 5.0}                                                                                                                                              
{'eval_loss': 3.165823727613315e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1241, 'eval_samples_per_second': 152.534, 'eval_steps_per_second': 2.825, 'epoch': 6.0}                                                                                                                                             
{'train_runtime': 284.9621, 'train_samples_per_second': 85.871, 'train_steps_per_second': 0.702, 'train_loss': 0.07444801005379607, 'epoch': 6.0}                                     
 60%|█████████████████████████████████████████████████████████████████████████████████████▊                                                         | 120/200 [04:44<03:09,  2.37s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.75it/s]
[I 2025-11-09 14:50:25,207] Trial 5 finished with value: 1.0 and parameters: {'lr': 3.292913906097766e-05, 'weight_decay': 0.0005614431736548964, 'warmup_ratio': 0.1529751717562691, 'batch_size': 32}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  30%|█████████████████████                                                 | 6/20 [27:09<1:06:56, 286.87s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.794, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.05}                                                                     
{'eval_loss': 0.09325680136680603, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 2.5412, 'eval_samples_per_second': 127.497, 'eval_steps_per_second': 2.361, 'epoch': 1.0}                                                                                                                                
{'eval_loss': 0.03069099597632885, 'eval_precision': 0.8872180451127819, 'eval_recall': 0.7283950617283951, 'eval_f1': 0.8, 'eval_accuracy': 0.9873702236968853, 'eval_runtime': 2.6311, 'eval_samples_per_second': 123.143, 'eval_steps_per_second': 2.28, 'epoch': 2.0}                                                                                                   
{'loss': 0.219, 'grad_norm': 0.22510981559753418, 'learning_rate': 1.04966765700219e-05, 'epoch': 2.52}                                                                               
{'eval_loss': 0.012069566175341606, 'eval_precision': 0.9228571428571428, 'eval_recall': 0.9969135802469136, 'eval_f1': 0.9584569732937686, 'eval_accuracy': 0.9970031039280745, 'eval_runtime': 2.2428, 'eval_samples_per_second': 144.462, 'eval_steps_per_second': 2.675, 'epoch': 3.0}                                                                                  
{'eval_loss': 0.0007499069906771183, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1253, 'eval_samples_per_second': 152.449, 'eval_steps_per_second': 2.823, 'epoch': 4.0}                                                                                                                                             
{'loss': 0.0067, 'grad_norm': 0.022777030244469643, 'learning_rate': 7.020955851471602e-06, 'epoch': 5.0}                                                                             
{'eval_loss': 0.00012420726125128567, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.4951, 'eval_samples_per_second': 129.856, 'eval_steps_per_second': 2.405, 'epoch': 5.0}                                                                                                                                            
{'eval_loss': 8.277117012767121e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2105, 'eval_samples_per_second': 146.57, 'eval_steps_per_second': 2.714, 'epoch': 6.0}                                                                                                                                              
{'eval_loss': 6.948582449695095e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.4744, 'eval_samples_per_second': 130.941, 'eval_steps_per_second': 2.425, 'epoch': 7.0}                                                                                                                                             
{'train_runtime': 378.33, 'train_samples_per_second': 64.679, 'train_steps_per_second': 0.529, 'train_loss': 0.0849750671003546, 'epoch': 7.0}                                        
 70%|████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 140/200 [06:18<02:42,  2.70s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.50it/s]
[I 2025-11-09 14:56:55,790] Trial 6 finished with value: 1.0 and parameters: {'lr': 1.306870990174912e-05, 'weight_decay': 0.04358076377461321, 'warmup_ratio': 0.05767211084673218, 'batch_size': 32}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  35%|████████████████████████▌                                             | 7/20 [33:40<1:09:30, 320.78s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.8774, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.05}                                                                    
{'eval_loss': 0.09149903059005737, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 2.1976, 'eval_samples_per_second': 147.431, 'eval_steps_per_second': 2.73, 'epoch': 1.0}                                                                                                                                 
{'eval_loss': 0.02084536664187908, 'eval_precision': 0.8526315789473684, 'eval_recall': 1.0, 'eval_f1': 0.9204545454545454, 'eval_accuracy': 0.994006207856149, 'eval_runtime': 2.2425, 'eval_samples_per_second': 144.479, 'eval_steps_per_second': 2.676, 'epoch': 2.0}                                                                                                   
{'loss': 0.2045, 'grad_norm': 0.23885665833950043, 'learning_rate': 2.0760390657919995e-05, 'epoch': 2.52}                                                                            
{'eval_loss': 0.0001509689900558442, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.8684, 'eval_samples_per_second': 112.954, 'eval_steps_per_second': 2.092, 'epoch': 3.0}                                                                                                                                             
{'eval_loss': 5.8948233345290646e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5923, 'eval_samples_per_second': 124.986, 'eval_steps_per_second': 2.315, 'epoch': 4.0}                                                                                                                                            
{'loss': 0.001, 'grad_norm': 0.010152335278689861, 'learning_rate': 1.3886089115562381e-05, 'epoch': 5.0}                                                                             
{'eval_loss': 5.3032923460705206e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2312, 'eval_samples_per_second': 145.216, 'eval_steps_per_second': 2.689, 'epoch': 5.0}                                                                                                                                            
{'eval_loss': 4.3356067180866376e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.3283, 'eval_samples_per_second': 139.157, 'eval_steps_per_second': 2.577, 'epoch': 6.0}                                                                                                                                            
{'train_runtime': 318.7086, 'train_samples_per_second': 76.779, 'train_steps_per_second': 0.628, 'train_loss': 0.091327611845918, 'epoch': 6.0}                                       
 60%|█████████████████████████████████████████████████████████████████████████████████████▊                                                         | 120/200 [05:18<03:32,  2.66s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.43it/s]
[I 2025-11-09 15:02:23,082] Trial 7 finished with value: 1.0 and parameters: {'lr': 2.5434915706723173e-05, 'weight_decay': 0.011846032500505133, 'warmup_ratio': 0.07183228925140639, 'batch_size': 32}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  40%|████████████████████████████                                          | 8/20 [39:07<1:04:34, 322.85s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.794, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.05}                                                                     
{'eval_loss': 0.10038816183805466, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 2.4538, 'eval_samples_per_second': 132.039, 'eval_steps_per_second': 2.445, 'epoch': 1.0}                                                                                                                                
{'eval_loss': 0.024086372926831245, 'eval_precision': 0.8496042216358839, 'eval_recall': 0.9938271604938271, 'eval_f1': 0.9160739687055477, 'eval_accuracy': 0.9936851118484427, 'eval_runtime': 2.4504, 'eval_samples_per_second': 132.221, 'eval_steps_per_second': 2.449, 'epoch': 2.0}                                                                                  
{'loss': 0.222, 'grad_norm': 0.2885344922542572, 'learning_rate': 1.4703206046722083e-05, 'epoch': 2.52}                                                                              
{'eval_loss': 0.0022332980297505856, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2139, 'eval_samples_per_second': 146.351, 'eval_steps_per_second': 2.71, 'epoch': 3.0}                                                                                                                                              
{'eval_loss': 0.00010344664042349905, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.671, 'eval_samples_per_second': 121.303, 'eval_steps_per_second': 2.246, 'epoch': 4.0}                                                                                                                                             
{'loss': 0.0031, 'grad_norm': 0.01853904500603676, 'learning_rate': 9.834594772973048e-06, 'epoch': 5.0}                                                                              
{'eval_loss': 5.814098039991222e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.209, 'eval_samples_per_second': 146.673, 'eval_steps_per_second': 2.716, 'epoch': 5.0}                                                                                                                                              
{'eval_loss': 5.293458889354952e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2301, 'eval_samples_per_second': 145.284, 'eval_steps_per_second': 2.69, 'epoch': 6.0}                                                                                                                                              
{'train_runtime': 283.5335, 'train_samples_per_second': 86.304, 'train_steps_per_second': 0.705, 'train_loss': 0.09862798055013021, 'epoch': 6.0}                                     
 60%|█████████████████████████████████████████████████████████████████████████████████████▊                                                         | 120/200 [04:43<03:09,  2.36s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.57it/s]
[I 2025-11-09 15:07:14,787] Trial 8 finished with value: 1.0 and parameters: {'lr': 1.7721745036446482e-05, 'weight_decay': 0.0961025453159905, 'warmup_ratio': 0.0899746961237763, 'batch_size': 32}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  45%|████████████████████████████████▍                                       | 9/20 [43:59<57:24, 313.11s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.8057, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.01}                                                                    
{'loss': 0.3276, 'grad_norm': 0.4093688130378723, 'learning_rate': 1.9415092268112448e-05, 'epoch': 0.65}                                                                             
{'eval_loss': 0.0006401240243576467, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1509, 'eval_samples_per_second': 150.632, 'eval_steps_per_second': 9.763, 'epoch': 1.0}                                                                                                                                             
{'loss': 0.0089, 'grad_norm': 0.5248404741287231, 'learning_rate': 3.922641090904351e-05, 'epoch': 1.3}                                                                               
{'loss': 0.0012, 'grad_norm': 0.01898781768977642, 'learning_rate': 5.903772954997458e-05, 'epoch': 1.95}                                                                             
{'eval_loss': 0.0006326642469502985, 'eval_precision': 0.9969230769230769, 'eval_recall': 1.0, 'eval_f1': 0.9984591679506933, 'eval_accuracy': 0.9998929679974312, 'eval_runtime': 2.2991, 'eval_samples_per_second': 140.925, 'eval_steps_per_second': 9.134, 'epoch': 2.0}                                                                                                
{'loss': 0.001, 'grad_norm': 0.0027483939193189144, 'learning_rate': 7.308266094387177e-05, 'epoch': 2.6}                                                                             
{'eval_loss': 1.5465326214325614e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.7742, 'eval_samples_per_second': 116.79, 'eval_steps_per_second': 7.57, 'epoch': 3.0}                                                                                                                                              
{'loss': 0.0006, 'grad_norm': 0.008345670998096466, 'learning_rate': 6.668312846192152e-05, 'epoch': 3.25}                                                                            
{'loss': 0.0149, 'grad_norm': 1.1923030614852905, 'learning_rate': 6.028359597997128e-05, 'epoch': 3.9}                                                                               
{'eval_loss': 0.049967605620622635, 'eval_precision': 1.0, 'eval_recall': 0.7160493827160493, 'eval_f1': 0.8345323741007195, 'eval_accuracy': 0.9901530557636733, 'eval_runtime': 2.6356, 'eval_samples_per_second': 122.933, 'eval_steps_per_second': 7.968, 'epoch': 4.0}                                                                                                 
{'train_runtime': 247.2638, 'train_samples_per_second': 98.963, 'train_steps_per_second': 3.114, 'train_loss': 0.0599970995726717, 'epoch': 4.0}                                      
 40%|█████████████████████████████████████████████████████████▏                                                                                     | 308/770 [04:07<06:10,  1.25it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:01<00:00, 20.30it/s]
[I 2025-11-09 15:11:32,161] Trial 9 finished with value: 1.0 and parameters: {'lr': 7.449055808990082e-05, 'weight_decay': 0.08966408687413371, 'warmup_ratio': 0.24384888365606452, 'batch_size': 8}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  50%|███████████████████████████████████▌                                   | 10/20 [48:16<49:19, 295.91s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.5855, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.03}                                                                    
{'eval_loss': 0.034251801669597626, 'eval_precision': 0.8140703517587939, 'eval_recall': 1.0, 'eval_f1': 0.8975069252077562, 'eval_accuracy': 0.9920796318099112, 'eval_runtime': 2.6148, 'eval_samples_per_second': 123.91, 'eval_steps_per_second': 4.207, 'epoch': 1.0}                                                                                                  
{'loss': 0.1791, 'grad_norm': 0.26337626576423645, 'learning_rate': 5.988373999645211e-05, 'epoch': 1.29}                                                                             
{'eval_loss': 3.5094257327727973e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 1.9065, 'eval_samples_per_second': 169.949, 'eval_steps_per_second': 5.77, 'epoch': 2.0}                                                                                                                                             
{'loss': 0.0014, 'grad_norm': 0.022765744477510452, 'learning_rate': 8.189726960307938e-05, 'epoch': 2.58}                                                                            
{'eval_loss': 4.8667181545170024e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2938, 'eval_samples_per_second': 141.252, 'eval_steps_per_second': 4.796, 'epoch': 3.0}                                                                                                                                            
{'loss': 0.0007, 'grad_norm': 0.03535959869623184, 'learning_rate': 6.782557379499015e-05, 'epoch': 3.86}                                                                             
{'eval_loss': 3.9423885027645156e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6348, 'eval_samples_per_second': 122.969, 'eval_steps_per_second': 4.175, 'epoch': 4.0}                                                                                                                                            
{'eval_loss': 1.906565739773214e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.3557, 'eval_samples_per_second': 137.54, 'eval_steps_per_second': 4.67, 'epoch': 5.0}                                                                                                                                               
{'train_runtime': 275.1208, 'train_samples_per_second': 88.943, 'train_steps_per_second': 1.418, 'train_loss': 0.048597822562815286, 'epoch': 5.0}                                    
 50%|███████████████████████████████████████████████████████████████████████▌                                                                       | 195/390 [04:35<04:35,  1.41s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 12.51it/s]
[I 2025-11-09 15:16:19,139] Trial 10 finished with value: 1.0 and parameters: {'lr': 8.92145514232858e-05, 'weight_decay': 0.06619143434917749, 'warmup_ratio': 0.18503427566180464, 'batch_size': 16}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  55%|███████████████████████████████████████                                | 11/20 [53:03<43:58, 293.17s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6088, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.03}                                                                    
{'eval_loss': 0.009017612785100937, 'eval_precision': 0.9283667621776505, 'eval_recall': 1.0, 'eval_f1': 0.962852897473997, 'eval_accuracy': 0.9973241999357808, 'eval_runtime': 2.5041, 'eval_samples_per_second': 129.386, 'eval_steps_per_second': 4.393, 'epoch': 1.0}                                                                                                  
{'loss': 0.1032, 'grad_norm': 0.18840435147285461, 'learning_rate': 1.760029616160884e-05, 'epoch': 1.29}                                                                             
{'eval_loss': 9.869992209132761e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 1.9805, 'eval_samples_per_second': 163.595, 'eval_steps_per_second': 5.554, 'epoch': 2.0}                                                                                                                                             
{'loss': 0.0011, 'grad_norm': 0.16415083408355713, 'learning_rate': 1.5019607574862674e-05, 'epoch': 2.58}                                                                            
{'eval_loss': 5.363488526199944e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.19, 'eval_samples_per_second': 147.942, 'eval_steps_per_second': 5.023, 'epoch': 3.0}                                                                                                                                               
{'loss': 0.0004, 'grad_norm': 0.01210771594196558, 'learning_rate': 1.243891898811651e-05, 'epoch': 3.86}                                                                             
{'eval_loss': 3.662255403469317e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1055, 'eval_samples_per_second': 153.882, 'eval_steps_per_second': 5.224, 'epoch': 4.0}                                                                                                                                             
{'eval_loss': 2.8788330382667482e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5513, 'eval_samples_per_second': 126.992, 'eval_steps_per_second': 4.311, 'epoch': 5.0}                                                                                                                                            
{'train_runtime': 223.5652, 'train_samples_per_second': 109.454, 'train_steps_per_second': 1.744, 'train_loss': 0.029517341003968165, 'epoch': 5.0}                                   
 50%|███████████████████████████████████████████████████████████████████████▌                                                                       | 195/390 [03:43<03:43,  1.15s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 13.36it/s]
[I 2025-11-09 15:20:14,520] Trial 11 finished with value: 1.0 and parameters: {'lr': 1.997452966141531e-05, 'weight_decay': 0.06699313614432797, 'warmup_ratio': 0.007657843172551887, 'batch_size': 16}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  60%|██████████████████████████████████████████▌                            | 12/20 [56:59<36:44, 275.59s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6088, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.03}                                                                    
{'eval_loss': 0.028992753475904465, 'eval_precision': 0.8851963746223565, 'eval_recall': 0.904320987654321, 'eval_f1': 0.8946564885496183, 'eval_accuracy': 0.992614791822755, 'eval_runtime': 2.1645, 'eval_samples_per_second': 149.686, 'eval_steps_per_second': 5.082, 'epoch': 1.0}                                                                                    
{'loss': 0.207, 'grad_norm': 0.3837467133998871, 'learning_rate': 3.369514983859103e-05, 'epoch': 1.29}                                                                               
{'eval_loss': 0.001123076886869967, 'eval_precision': 1.0, 'eval_recall': 0.9969135802469136, 'eval_f1': 0.9984544049459042, 'eval_accuracy': 0.9998929679974312, 'eval_runtime': 2.1541, 'eval_samples_per_second': 150.414, 'eval_steps_per_second': 5.107, 'epoch': 2.0}                                                                                                 
{'loss': 0.0012, 'grad_norm': 0.2856919765472412, 'learning_rate': 5.327548803660608e-05, 'epoch': 2.58}                                                                              
{'eval_loss': 4.573233672999777e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.4946, 'eval_samples_per_second': 129.882, 'eval_steps_per_second': 4.41, 'epoch': 3.0}                                                                                                                                              
{'loss': 0.0004, 'grad_norm': 0.019002821296453476, 'learning_rate': 4.412162411279061e-05, 'epoch': 3.86}                                                                            
{'eval_loss': 1.1994097803835757e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5906, 'eval_samples_per_second': 125.065, 'eval_steps_per_second': 4.246, 'epoch': 4.0}                                                                                                                                            
{'eval_loss': 1.53731343743857e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.3091, 'eval_samples_per_second': 140.316, 'eval_steps_per_second': 4.764, 'epoch': 5.0}                                                                                                                                              
{'loss': 0.0007, 'grad_norm': 0.0022516059689223766, 'learning_rate': 3.496776018897513e-05, 'epoch': 5.13}                                                                           
{'eval_loss': 1.4815558643022086e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.3459, 'eval_samples_per_second': 138.111, 'eval_steps_per_second': 4.689, 'epoch': 6.0}                                                                                                                                            
{'train_runtime': 344.2136, 'train_samples_per_second': 71.09, 'train_steps_per_second': 1.133, 'train_loss': 0.0464535107016245, 'epoch': 6.0}                                       
 60%|█████████████████████████████████████████████████████████████████████████████████████▊                                                         | 234/390 [05:44<03:49,  1.47s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 12.77it/s]
[I 2025-11-09 15:26:08,006] Trial 12 finished with value: 1.0 and parameters: {'lr': 5.638780177070335e-05, 'weight_decay': 0.03316755996320203, 'warmup_ratio': 0.20952881739207613, 'batch_size': 16}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  65%|████████████████████████████████████████████▊                        | 13/20 [1:02:52<34:54, 299.19s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.8036, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.03}                                                                    
{'eval_loss': 0.27005425095558167, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 2.5081, 'eval_samples_per_second': 129.183, 'eval_steps_per_second': 4.386, 'epoch': 1.0}                                                                                                                                
{'loss': 0.5411, 'grad_norm': 1.7866817712783813, 'learning_rate': 5.102565895030013e-06, 'epoch': 1.29}                                                                              
{'eval_loss': 0.03351467475295067, 'eval_precision': 0.8976608187134503, 'eval_recall': 0.9475308641975309, 'eval_f1': 0.9219219219219219, 'eval_accuracy': 0.994434335866424, 'eval_runtime': 2.4241, 'eval_samples_per_second': 133.66, 'eval_steps_per_second': 4.538, 'epoch': 2.0}                                                                                     
{'loss': 0.0426, 'grad_norm': 0.3210880160331726, 'learning_rate': 1.0309265787917782e-05, 'epoch': 2.58}                                                                             
{'eval_loss': 0.0008324364316649735, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 1.9264, 'eval_samples_per_second': 168.188, 'eval_steps_per_second': 5.71, 'epoch': 3.0}                                                                                                                                              
{'loss': 0.0028, 'grad_norm': 1.97601318359375, 'learning_rate': 1.0494813638646144e-05, 'epoch': 3.86}                                                                               
{'eval_loss': 8.477805386064574e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5582, 'eval_samples_per_second': 126.651, 'eval_steps_per_second': 4.3, 'epoch': 4.0}                                                                                                                                               
{'eval_loss': 6.447405758081004e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6113, 'eval_samples_per_second': 124.075, 'eval_steps_per_second': 4.212, 'epoch': 5.0}                                                                                                                                             
{'loss': 0.0007, 'grad_norm': 0.014658492058515549, 'learning_rate': 8.31746641071126e-06, 'epoch': 5.13}                                                                             
{'eval_loss': 4.605025605997071e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1264, 'eval_samples_per_second': 152.373, 'eval_steps_per_second': 5.173, 'epoch': 6.0}                                                                                                                                             
{'train_runtime': 294.9978, 'train_samples_per_second': 82.95, 'train_steps_per_second': 1.322, 'train_loss': 0.12668827178482062, 'epoch': 6.0}                                      
 60%|█████████████████████████████████████████████████████████████████████████████████████▊                                                         | 234/390 [04:54<03:16,  1.26s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 12.93it/s]
[I 2025-11-09 15:31:11,055] Trial 13 finished with value: 1.0 and parameters: {'lr': 1.1975409753641866e-05, 'weight_decay': 0.07825214212526603, 'warmup_ratio': 0.29371983210149694, 'batch_size': 16}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  70%|████████████████████████████████████████████████▎                    | 14/20 [1:07:55<30:02, 300.35s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.8036, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.03}                                                                    
{'eval_loss': 0.046907391399145126, 'eval_precision': 1.0, 'eval_recall': 0.09876543209876543, 'eval_f1': 0.1797752808988764, 'eval_accuracy': 0.9687466552499198, 'eval_runtime': 2.4899, 'eval_samples_per_second': 130.126, 'eval_steps_per_second': 4.418, 'epoch': 1.0}                                                                                                
{'loss': 0.3022, 'grad_norm': 0.4641556143760681, 'learning_rate': 2.22801624185511e-05, 'epoch': 1.29}                                                                               
{'eval_loss': 0.00010467970423633233, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2178, 'eval_samples_per_second': 146.093, 'eval_steps_per_second': 4.96, 'epoch': 2.0}                                                                                                                                             
{'loss': 0.0038, 'grad_norm': 0.13980093598365784, 'learning_rate': 1.990607973353301e-05, 'epoch': 2.58}                                                                             
{'eval_loss': 4.0104550862452015e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.1396, 'eval_samples_per_second': 151.427, 'eval_steps_per_second': 5.141, 'epoch': 3.0}                                                                                                                                            
{'loss': 0.0007, 'grad_norm': 0.008978725410997868, 'learning_rate': 1.6485791119523904e-05, 'epoch': 3.86}                                                                           
{'eval_loss': 2.5184710466419347e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2759, 'eval_samples_per_second': 142.362, 'eval_steps_per_second': 4.833, 'epoch': 4.0}                                                                                                                                            
{'eval_loss': 1.7849242794909514e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.4484, 'eval_samples_per_second': 132.331, 'eval_steps_per_second': 4.493, 'epoch': 5.0}                                                                                                                                            
{'train_runtime': 235.7088, 'train_samples_per_second': 103.815, 'train_steps_per_second': 1.655, 'train_loss': 0.08125663013316882, 'epoch': 5.0}                                    
 50%|███████████████████████████████████████████████████████████████████████▌                                                                       | 195/390 [03:55<03:55,  1.21s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 12.90it/s]
[I 2025-11-09 15:35:16,107] Trial 14 finished with value: 1.0 and parameters: {'lr': 2.3189556802981754e-05, 'weight_decay': 0.05536640236671851, 'warmup_ratio': 0.13006959585027839, 'batch_size': 16}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  75%|███████████████████████████████████████████████████▊                 | 15/20 [1:12:00<23:38, 283.68s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6089, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.01}                                                                    
{'loss': 0.3131, 'grad_norm': 0.6388323903083801, 'learning_rate': 1.2765057650572842e-05, 'epoch': 0.65}                                                                             
{'eval_loss': 0.00131930410861969, 'eval_precision': 0.9938650306748467, 'eval_recall': 1.0, 'eval_f1': 0.9969230769230769, 'eval_accuracy': 0.9997859359948624, 'eval_runtime': 2.2847, 'eval_samples_per_second': 141.81, 'eval_steps_per_second': 9.191, 'epoch': 1.0}                                                                                                   
{'loss': 0.0114, 'grad_norm': 1.321746826171875, 'learning_rate': 2.5790626681769614e-05, 'epoch': 1.3}                                                                               
Best trial: 0. Best value: 1:  75%|███████████████████████████████████████████████████▊                 | 15/20 [1:13:25<23:38, 283.68s/it]                                           
 19%|██████████████████████████▌                                                                                                                    | 143/770 
 19%|██████████████████████████▋                                                                                                                    | 144/770 
 19%|██████████████████████▍                                                                                                | 145/770 [01:40<05:12,  2.00it/s]
 19%|██████████████████████▋                                                                                                | 147/770 [
 19%|██████████████████████▊                                                                                                | 148/770 [
                                                                                                                                       {'loss': 0.0012, 'grad_norm': 0.022337045520544052, 'learning_rate': 3.881619571296639e-05, 'epoch': 1.95}/770 [01:43<05:13,  1.98it/s]
Best trial: 0. Best value: 1:  75%|███████████████████████████████████████████████████▊                 | 15/20 [1:13:50<23:38, 283.68s{'eval_loss': 6.168535765027627e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.3066, 'eval_samples_per_second': 140.467, 'eval_steps_per_second': 9.104, 'epoch': 2.0}                                                                                                                                     
{'loss': 0.0004, 'grad_norm': 0.01148129627108574, 'learning_rate': 4.8729102903259936e-05, 'epoch': 2.6}                                                                         
{'eval_loss': 1.5531726603512652e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.4287, 'eval_samples_per_second': 133.403, 'eval_steps_per_second': 8.646, 'epoch': 3.0}                                                                                                                                    
{'loss': 0.0001, 'grad_norm': 0.0022339036222547293, 'learning_rate': 4.446210615166099e-05, 'epoch': 3.25}                                                                       
{'loss': 0.0006, 'grad_norm': 0.8334080576896667, 'learning_rate': 4.019510940006205e-05, 'epoch': 3.9}                                                                           
{'eval_loss': 0.002402959857136011, 'eval_precision': 0.9878048780487805, 'eval_recall': 1.0, 'eval_f1': 0.9938650306748467, 'eval_accuracy': 0.999571871989725, 'eval_runtime': 2.7275, 'eval_samples_per_second': 118.792, 'eval_steps_per_second': 7.699, 'epoch': 4.0}                                                                                          
{'loss': 0.0015, 'grad_norm': 0.004190964624285698, 'learning_rate': 3.59281126484631e-05, 'epoch': 4.55}                                                                         
{'eval_loss': 2.1018824554630555e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6813, 'eval_samples_per_second': 120.835, 'eval_steps_per_second': 7.832, 'epoch': 5.0}                                                                                                                                    
{'train_runtime': 334.4463, 'train_samples_per_second': 73.166, 'train_steps_per_second': 2.302, 'train_loss': 0.043428938931410574, 'epoch': 5.0}                                
 50%|█████████████████████████████████████████████████████████████████████▌                                                                     | 385/770 [05:34<05:34,  1.15it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 21.29it/s]
[I 2025-11-09 15:41:01,202] Trial 15 finished with value: 1.0 and parameters: {'lr': 4.949716231854775e-05, 'weight_decay': 0.030237060899430356, 'warmup_ratio': 0.2460908868274801, 'batch_size': 8}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  80%|███████████████████████████████████████████████████████▏             | 16/20 [1:17:45<20:08, 302.17s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6088, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.03}                                                                    
{'eval_loss': 0.09458345919847488, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 2.5125, 'eval_samples_per_second': 128.954, 'eval_steps_per_second': 4.378, 'epoch': 1.0}                                                                                                                        
{'loss': 0.3208, 'grad_norm': 0.5942807197570801, 'learning_rate': 1.1516252886785378e-05, 'epoch': 1.29}                                                                         
{'eval_loss': 0.0009551923722028732, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 1.8185, 'eval_samples_per_second': 178.172, 'eval_steps_per_second': 6.049, 'epoch': 2.0}                                                                                                                                     
{'loss': 0.0114, 'grad_norm': 0.13818024098873138, 'learning_rate': 1.4443124104684312e-05, 'epoch': 2.58}                                                                        
{'eval_loss': 8.946526941144839e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5735, 'eval_samples_per_second': 125.899, 'eval_steps_per_second': 4.274, 'epoch': 3.0}                                                                                                                                     
{'loss': 0.0006, 'grad_norm': 0.563316822052002, 'learning_rate': 1.1961487660580479e-05, 'epoch': 3.86}                                                                          
{'eval_loss': 6.042922177584842e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5016, 'eval_samples_per_second': 129.517, 'eval_steps_per_second': 4.397, 'epoch': 4.0}                                                                                                                                     
{'eval_loss': 4.8798076022649184e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5672, 'eval_samples_per_second': 126.206, 'eval_steps_per_second': 4.285, 'epoch': 5.0}                                                                                                                                    
{'train_runtime': 218.8059, 'train_samples_per_second': 111.834, 'train_steps_per_second': 1.782, 'train_loss': 0.08695518373487851, 'epoch': 5.0}                                
 50%|█████████████████████████████████████████████████████████████████████▌                                                                     | 195/390 [03:38<03:38,  1.12s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 12.58it/s]
[I 2025-11-09 15:44:48,630] Trial 16 finished with value: 1.0 and parameters: {'lr': 1.598173870002869e-05, 'weight_decay': 0.08186029233583304, 'warmup_ratio': 0.17188036151757477, 'batch_size': 16}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  85%|██████████████████████████████████████████████████████████▋          | 17/20 [1:21:33<13:59, 279.69s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6153, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.05}                                                                    
{'eval_loss': 0.007786603644490242, 'eval_precision': 0.963963963963964, 'eval_recall': 0.9907407407407407, 'eval_f1': 0.9771689497716894, 'eval_accuracy': 0.9983945199614684, 'eval_runtime': 2.1865, 'eval_samples_per_second': 148.184, 'eval_steps_per_second': 2.744, 'epoch': 1.0}                                                                           
{'eval_loss': 5.384851829148829e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2184, 'eval_samples_per_second': 146.049, 'eval_steps_per_second': 2.705, 'epoch': 2.0}                                                                                                                                     
{'loss': 0.0516, 'grad_norm': 0.2686009109020233, 'learning_rate': 4.639213311968477e-05, 'epoch': 2.52}                                                                          
{'eval_loss': 3.691239544423297e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 1.9915, 'eval_samples_per_second': 162.69, 'eval_steps_per_second': 3.013, 'epoch': 3.0}                                                                                                                                      
{'eval_loss': 1.9442893972154707e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.565, 'eval_samples_per_second': 126.317, 'eval_steps_per_second': 2.339, 'epoch': 4.0}                                                                                                                                     
{'loss': 0.0003, 'grad_norm': 0.0031927800737321377, 'learning_rate': 3.103049963634544e-05, 'epoch': 5.0}                                                                        
{'eval_loss': 1.653990511840675e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6008, 'eval_samples_per_second': 124.575, 'eval_steps_per_second': 2.307, 'epoch': 5.0}                                                                                                                                     
{'train_runtime': 232.2899, 'train_samples_per_second': 105.342, 'train_steps_per_second': 0.861, 'train_loss': 0.03157446319237352, 'epoch': 5.0}                                
 50%|█████████████████████████████████████████████████████████████████████▌                                                                     | 100/200 [03:52<03:52,  2.32s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.42it/s]
[I 2025-11-09 15:48:48,664] Trial 17 finished with value: 1.0 and parameters: {'lr': 6.0832068594023734e-05, 'weight_decay': 0.05913250130004473, 'warmup_ratio': 0.009719576503500998, 'batch_size': 32}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  90%|██████████████████████████████████████████████████████████████       | 18/20 [1:25:33<08:55, 267.78s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6088, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.03}                                                                    
{'eval_loss': 0.06361494958400726, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9653216311677192, 'eval_runtime': 2.2583, 'eval_samples_per_second': 143.473, 'eval_steps_per_second': 4.871, 'epoch': 1.0}                                                                                                                        
{'loss': 0.2702, 'grad_norm': 0.4224306643009186, 'learning_rate': 1.7746103116767497e-05, 'epoch': 1.29}                                                                         
{'eval_loss': 0.00014057733642403036, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6984, 'eval_samples_per_second': 120.069, 'eval_steps_per_second': 4.076, 'epoch': 2.0}                                                                                                                                    
{'loss': 0.0048, 'grad_norm': 0.12592743337154388, 'learning_rate': 3.070970459617395e-05, 'epoch': 2.58}                                                                         
{'eval_loss': 4.498484850046225e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.239, 'eval_samples_per_second': 144.707, 'eval_steps_per_second': 4.913, 'epoch': 3.0}                                                                                                                                      
{'loss': 0.0005, 'grad_norm': 0.015240771695971489, 'learning_rate': 2.543312305043959e-05, 'epoch': 3.86}                                                                        
{'eval_loss': 2.6487377908779308e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.208, 'eval_samples_per_second': 146.736, 'eval_steps_per_second': 4.982, 'epoch': 4.0}                                                                                                                                     
{'eval_loss': 2.504177246009931e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 1.9361, 'eval_samples_per_second': 167.344, 'eval_steps_per_second': 5.681, 'epoch': 5.0}                                                                                                                                     
{'train_runtime': 256.5768, 'train_samples_per_second': 95.371, 'train_steps_per_second': 1.52, 'train_loss': 0.07244663539891824, 'epoch': 5.0}                                  
 50%|█████████████████████████████████████████████████████████████████████▌                                                                     | 195/390 [04:16<04:16,  1.32s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 13.24it/s]
[I 2025-11-09 15:53:14,672] Trial 18 finished with value: 1.0 and parameters: {'lr': 3.1870552536235507e-05, 'weight_decay': 0.021246438406281837, 'warmup_ratio': 0.2247088150776015, 'batch_size': 16}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1:  95%|█████████████████████████████████████████████████████████████████▌   | 19/20 [1:29:59<04:27, 267.25s/it]Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6089, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.01}                                                                    
{'loss': 0.3149, 'grad_norm': 0.6561512351036072, 'learning_rate': 1.2586348790646853e-05, 'epoch': 0.65}                                                                         
{'eval_loss': 0.001559132244437933, 'eval_precision': 0.9938650306748467, 'eval_recall': 1.0, 'eval_f1': 0.9969230769230769, 'eval_accuracy': 0.9997859359948624, 'eval_runtime': 2.4081, 'eval_samples_per_second': 134.545, 'eval_steps_per_second': 8.721, 'epoch': 1.0}                                                                                         
{'loss': 0.0117, 'grad_norm': 1.0821173191070557, 'learning_rate': 2.5429561842327318e-05, 'epoch': 1.3}                                                                          
{'loss': 0.0009, 'grad_norm': 0.01834438554942608, 'learning_rate': 2.3807866582368856e-05, 'epoch': 1.95}                                                                        
{'eval_loss': 5.4659896704833955e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.7528, 'eval_samples_per_second': 117.699, 'eval_steps_per_second': 7.629, 'epoch': 2.0}                                                                                                                                    
{'loss': 0.0004, 'grad_norm': 0.007911576889455318, 'learning_rate': 2.189096911196879e-05, 'epoch': 2.6}                                                                         
{'eval_loss': 3.017072413058486e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.4331, 'eval_samples_per_second': 133.163, 'eval_steps_per_second': 8.631, 'epoch': 3.0}                                                                                                                                     
{'loss': 0.0005, 'grad_norm': 0.0052359821274876595, 'learning_rate': 1.997407164156872e-05, 'epoch': 3.25}                                                                       
{'loss': 0.0002, 'grad_norm': 0.0027059572748839855, 'learning_rate': 1.805717417116865e-05, 'epoch': 3.9}                                                                        
{'eval_loss': 1.664167757553514e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6562, 'eval_samples_per_second': 121.978, 'eval_steps_per_second': 7.906, 'epoch': 4.0}                                                                                                                                     
{'loss': 0.0001, 'grad_norm': 0.001128329779021442, 'learning_rate': 1.614027670076858e-05, 'epoch': 4.55}                                                                        
{'eval_loss': 1.2971359865332488e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.5851, 'eval_samples_per_second': 125.336, 'eval_steps_per_second': 8.124, 'epoch': 5.0}                                                                                                                                    
{'train_runtime': 413.8122, 'train_samples_per_second': 59.133, 'train_steps_per_second': 1.861, 'train_loss': 0.04346486703710413, 'epoch': 5.0}                                 
 50%|█████████████████████████████████████████████████████████████████████▌                                                                     | 385/770 [06:53<06:53,  1.07s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 21.32it/s]
[I 2025-11-09 16:00:19,288] Trial 19 finished with value: 1.0 and parameters: {'lr': 2.5686426103360926e-05, 'weight_decay': 0.00014198563316474618, 'warmup_ratio': 0.12931873658642568, 'batch_size': 8}. Best is trial 0 with value: 1.0.
Best trial: 0. Best value: 1: 100%|█████████████████████████████████████████████████████████████████████| 20/20 [1:37:03<00:00, 291.20s/it]

Best params: {'lr': 3.8417123927791296e-05, 'weight_decay': 0.07435023736314793, 'warmup_ratio': 0.23647663064662078, 'batch_size': 16}
Best F1: 1.0000

Training final model for xlm-roberta-base...
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /workspace/wandb/run-20251109_160019-n85uh2v6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run xlm-roberta-base_final
wandb: ⭐️ View project at https://wandb.ai/architk2003-sapienza-universit-di-roma/sentence_splitting_encoder
wandb: 🚀 View run at https://wandb.ai/architk2003-sapienza-universit-di-roma/sentence_splitting_encoder/runs/n85uh2v6
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.6089, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.01}                                                                                                           
{'loss': 0.2193, 'grad_norm': 0.7309690713882446, 'learning_rate': 3.181818181818182e-05, 'epoch': 0.65}                                                                          
{'eval_loss': 8.146821346599609e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.2304, 'eval_samples_per_second': 145.267, 'eval_steps_per_second': 9.415, 'epoch': 1.0}                                                                                                                                     
{'loss': 0.0022, 'grad_norm': 0.25446584820747375, 'learning_rate': 4.841269841269841e-05, 'epoch': 1.3}                                                                          
{'loss': 0.0032, 'grad_norm': 0.15712212026119232, 'learning_rate': 4.4805194805194805e-05, 'epoch': 1.95}                                                                        
{'eval_loss': 0.0005094666848890483, 'eval_precision': 0.9969230769230769, 'eval_recall': 1.0, 'eval_f1': 0.9984591679506933, 'eval_accuracy': 0.9998929679974312, 'eval_runtime': 2.5027, 'eval_samples_per_second': 129.462, 'eval_steps_per_second': 8.391, 'epoch': 2.0}                                                                                        
{'loss': 0.0009, 'grad_norm': 0.073956198990345, 'learning_rate': 4.1197691197691204e-05, 'epoch': 2.6}                                                                           
{'eval_loss': 1.9706483726622537e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.419, 'eval_samples_per_second': 133.939, 'eval_steps_per_second': 8.681, 'epoch': 3.0}                                                                                                                                     
{'loss': 0.0004, 'grad_norm': 0.001036530127748847, 'learning_rate': 3.759018759018759e-05, 'epoch': 3.25}                                                                        
{'loss': 0.0001, 'grad_norm': 0.0025594145990908146, 'learning_rate': 3.398268398268398e-05, 'epoch': 3.9}                                                                        
{'eval_loss': 6.512469099106966e-06, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 2.6662, 'eval_samples_per_second': 121.521, 'eval_steps_per_second': 7.876, 'epoch': 4.0}                                                                                                                                     
{'train_runtime': 257.7011, 'train_samples_per_second': 94.955, 'train_steps_per_second': 2.988, 'train_loss': 0.03797418694163056, 'epoch': 4.0}                                 
 40%|███████████████████████████████████████████████████████▌                                                                                   | 308/770 [04:17<06:26,  1.20it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 21.15it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.47it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 21.60it/s]

xlm-roberta-base Results:
  Dev F1: 1.0000
  Dev Acc: 1.0000
  OOD F1: 0.7565
  OOD Acc: 0.9629
wandb: 
wandb: Run history:
wandb:           eval/accuracy █████▁█
wandb:                 eval/f1 █████▁█
wandb:               eval/loss ▁▁▁▁▁█▁
wandb:          eval/precision █████▁█
wandb:             eval/recall █████▁█
wandb:            eval/runtime ▄▇▆█▅▁▃
wandb: eval/samples_per_second ▇▆▇▆▇▁█
wandb:   eval/steps_per_second ▇▆▇▆▇▁█
wandb:             train/epoch ▁▂▃▃▄▄▆▆▇██████
wandb:       train/global_step ▁▂▃▃▄▄▆▆▇████████
wandb:                     +15 ...
wandb: 
wandb: Run summary:
wandb:           eval/accuracy 1
wandb:                 eval/f1 1
wandb:               eval/loss 8e-05
wandb:          eval/precision 1
wandb:             eval/recall 1
wandb:            eval/runtime 2.0699
wandb: eval/samples_per_second 156.529
wandb:   eval/steps_per_second 10.145
wandb:              total_flos 2557572654931968.0
wandb:             train/epoch 4
wandb:                     +20 ...
wandb: 
wandb: 🚀 View run xlm-roberta-base_final at: https://wandb.ai/architk2003-sapienza-universit-di-roma/sentence_splitting_encoder/runs/n85uh2v6
wandb: ⭐️ View project at: https://wandb.ai/architk2003-sapienza-universit-di-roma/sentence_splitting_encoder
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251109_160019-n85uh2v6/logs

================================================================================
FINAL RESULTS
================================================================================
           Model  Dev F1  Dev Acc   OOD F1  OOD Acc  Best LR  Best WD
xlm-roberta-base     1.0      1.0 0.756522 0.962865 0.000038  0.07435
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /workspace/wandb/run-20251109_160503-23mciggn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run final_summary
wandb: ⭐️ View project at https://wandb.ai/architk2003-sapienza-universit-di-roma/sentence_splitting_encoder
wandb: 🚀 View run at https://wandb.ai/architk2003-sapienza-universit-di-roma/sentence_splitting_encoder/runs/23mciggn
wandb: 🚀 View run final_summary at: https://wandb.ai/architk2003-sapienza-universit-di-roma/sentence_splitting_encoder/runs/23mciggn
wandb: ⭐️ View project at: https://wandb.ai/architk2003-sapienza-universit-di-roma/sentence_splitting_encoder
wandb: Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251109_160503-23mciggn/logs

✅ Complete! Results in outputs_only_encoder
root@da7e5d60e8de:/# 